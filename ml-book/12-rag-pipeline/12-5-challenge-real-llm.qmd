# 12.5 Challenge: 连接真实LLM，打通端到端流程

在我们的实践中，`answer_question`函数在最后一步返回了精心构建的`final_prompt`，我们"模拟"了对LLM的调用。这对于验证流程至关重要，但我们离真正的、能自动回答问题的机器人，还差这"临门一脚"。

现在，是时候突破模拟，连接一个真实的大语言模型，见证你亲手创造的RAG系统"活"起来的瞬间了。

---

::: {.callout-warning title="开放性挑战：连接真实LLM，打通端到端流程"}

你的任务是：**修改`answer_question`函数，将最后一步的`return final_prompt`替换为真实的大语言模型API调用，并让函数直接返回LLM生成的最终答案字符串。**

这是一个高度开放的挑战，因为你可以选择任何你能够访问的LLM。我们强烈推荐一个对初学者非常友好的方案：**通过Ollama在你的本地电脑上运行开源LLM。**

**指令剧本大纲:**

**第一步：在本地运行LLM (推荐方案：Ollama)**
> **👤 你的指令 (可以问搜索引擎或AI助手):**
> "我如何在我的电脑上（Windows/macOS/Linux）安装和使用Ollama？请给我一个傻瓜式的教程。"
> "使用Ollama，我应该如何下载并运行一个轻量级的、适合进行聊天问答的开源模型？（例如 `llama3`, `qwen:7b-chat` 或 `gemma`）"
> "如何使用Python的`requests`库，向本地运行的Ollama模型API发送一个POST请求，并获取它的回答？请给我一个可以工作的代码片段。"

**第二步：修改你的RAG函数**
> **👤 你的指令 (对你的AI编程助手):**
> "你好，这是我当前的`answer_question`函数，它最后会返回一个`final_prompt`字符串。
> ```python
> # ... (粘贴你之前的函数代码) ...
> ```
> 现在，请帮我修改这个函数。在函数的最后，不要返回`final_prompt`。请添加代码，将这个`final_prompt`通过HTTP POST请求，发送到本地Ollama的API端点（通常是`http://localhost:11434/api/generate`），然后解析返回的JSON，提取出模型的回答内容，并将这个内容作为函数的最终返回值。"

**第三步：测试与验证**
-   运行你修改后的函数，向它提出一个你知道知识库里有答案的问题。
-   观察它返回的答案。这个答案是你期望的吗？它是否忠实于你提供的上下文？
-   **如果答案不理想，你会怎么做？**
    -   你会首先怀疑是**检索**阶段出的问题，还是**生成**阶段出的问题？
    -   你会去修改你的**Prompt模板**吗？你会如何修改？
    -   你会考虑更换一个不同的**本地LLM模型**吗？

**备选方案：使用商业LLM API**

如果你拥有OpenAI, Anthropic, Google Gemini, 或国内厂商（如智谱、文心等）的API密钥，你也可以直接使用它们的Python SDK来替换Ollama的部分。

> **👤 你的指令 (对你的AI编程助手):**
> "请帮我修改`answer_question`函数，在最后一步使用`openai` Python库，调用`ChatCompletion.create`接口，将`final_prompt`发送给GPT-3.5-turbo模型，并返回其生成的答案。"

这个挑战将让你完成从一个"系统构建者"到"端到端AI应用开发者"的最后一次蜕变。你将直面真实世界中模型表现的不确定性，并学会如何系统性地调试和优化一个完整的AI应用。
:::
