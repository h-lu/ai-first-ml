# 12.4 Practice: 完整的RAG"指令剧本"

我们已经分别打造好了所有独立的组件，现在，是时候将它们组装起来，完成这趟旅程中最激动人心的一步了。

我们将通过一个最终的"指令剧本"，指挥AI将我们所有的成果——文本处理、向量化、FAISS索引、Prompt模板——无缝地整合到一个单一的、可执行的Python函数中。

::: {.callout-note title="最终指令：组装RAG问答函数" icon="fas-cogs"}

**👤 你的指令:**

> 你好，请扮演一位资深的Python工程师，帮我编写一个名为 `answer_question` 的Python函数。
>
> **函数要求**:
> 1.  接收三个参数: `question` (字符串), `embedding_model` (我们的句嵌入模型), `faiss_index` (我们构建的FAISS索引), 和 `chunks` (包含所有文档块原文的列表)。
> 2.  **函数内部需要执行完整的RAG流程**：
>     a.  **向量化问题**: 使用 `embedding_model` 将 `question` 字符串转换为查询向量。
>     b.  **检索文档**: 使用 `faiss_index` 在向量数据库中搜索最相似的 **3个(k=3)** 文档块的索引。
>     c.  **拼接上下文**: 根据检索到的索引，从 `chunks` 列表中找到对应的文档块原文，并将它们用换行符 `\n---\n` 连接起来，形成一个完整的 `context` 字符串。
>     d.  **创建Prompt**: 使用我们在上一节设计的Prompt模板，并将 `context` 和 `question` 填入模板中，生成最终的 `final_prompt`。
>     e.  **调用LLM (模拟)**: 为了简化，我们这里先不真实调用LLM API。函数直接将 `final_prompt` **返回**，作为最终的输出。
> 3.  **代码需要有清晰的注释**。

:::

---

## AI生成的代码

AI助手会理解你的意图，并将所有部分优雅地封装起来。

```python
#| eval: false
import numpy as np

# 假设 embedding_model, faiss_index, 和 chunks
# 这些变量是根据前几章的实践已经准备好并加载到环境中的

def answer_question(question: str, embedding_model, faiss_index, chunks: list):
    """
    接收一个用户问题，执行完整的RAG流程，并返回最终的Prompt。

    Args:
        question (str): 用户的提问。
        embedding_model: 加载好的句嵌入模型。
        faiss_index: 构建好的FAISS索引。
        chunks (list): 包含所有文档块原文的列表。

    Returns:
        str: 准备好发送给LLM的最终Prompt。
    """
    # 1. 将问题向量化
    # 注意：FAISS需要一个2D的NumPy数组，所以我们reshape
    query_embedding = embedding_model.encode([question])
    query_embedding_np = np.array(query_embedding).astype('float32')
    
    # 如果你的FAISS索引是归一化的，这里也要对查询向量归一化
    faiss.normalize_L2(query_embedding_np)

    # 2. 在FAISS索引中检索最相关的3个文档块
    k = 3
    # .search方法返回距离和索引
    distances, indices = faiss_index.search(query_embedding_np, k)
    
    retrieved_indices = indices[0] # 获取第一个查询结果的索引

    # 3. 根据索引从原文列表中获取内容，并拼接上下文
    retrieved_chunks = [chunks[i] for i in retrieved_indices]
    context = "\n---\n".join(retrieved_chunks)

    # 4. 创建Prompt
    prompt_template = """
你是一个专业的AI知识库助手。

你的任务是根据下面提供的【上下文】信息，简洁并准确地回答用户的【问题】。

规则:
1.  严格基于【上下文】进行回答，不要依赖任何外部知识。
2.  如果【上下文】没有提供足够的信息，或者与问题无关，必须直接回答："根据我现有的资料，无法回答这个问题。"
3.  回答应保持客观、中立，不要包含个人观点或猜测。

---
【上下文】:
{context}
---

【问题】:
{question}
"""
    final_prompt = prompt_template.format(context=context, question=question)

    # 5. 返回最终的Prompt
    return final_prompt

# --- 运行我们的RAG流程 (示例) ---

# 假设这些变量已经存在
# embedding_model = ...
# faiss_index = ...
# chunks = [...]

# user_question = "RAG系统如何工作？"
# final_prompt_to_llm = answer_question(user_question, embedding_model, faiss_index, chunks)

# print("="*50)
# print("最终将发送给LLM的Prompt内容：")
# print(final_prompt_to_llm)
# print("="*50)

```

::: {.callout-note title="里程碑：一个完整的RAG引擎"}
>
> *   **见证奇迹**: 上面的代码，就是我们第二部分项目的核心产物。`answer_question`函数就像一个总指挥，有序地调度了Embedding、检索、文本处理和Prompt模板等所有资源，将它们协同成一个强大的RAG引擎。
> *   **从模拟到真实**: 我们在步骤 `5` 中返回了最终的Prompt。在真实的应用中，你只需要将这个`final_prompt`字符串，通过API（例如 `openai.ChatCompletion.create(...)`）发送给一个真实的LLM，就能获得最终的答案。
> *   **封装的力量**: 将复杂的流程封装到一个单一、接口清晰的函数中，是软件工程的最佳实践。现在，任何人都可以通过调用 `answer_question(...)` 来使用我们的整个RAG系统，而无需关心其内部复杂的实现细节。
:::

## 本节小结

### 🎯 核心收获
- **系统整合能力**: 你学会了如何将多个独立的AI和数据处理组件（文本分块、嵌入模型、向量索引、Prompt模板）整合到一个单一、高内聚的函数中。
- **RAG引擎封装**: 你亲手构建并理解了一个完整的RAG查询引擎 (`answer_question` 函数) 的内部工作流程。
- **从模拟到真实**: 你明白了在真实的RAG应用中，最后一步就是将精心构造的Prompt发送给一个大语言模型。

### 🤔 下一步的思考
- **模拟的局限**: 我们的`answer_question`函数目前只生成了Prompt，并没有真正地调用LLM来获得答案。这就像是万事俱备，只欠东风。
- **真实世界的挑战**: 如果我们真的接入一个LLM，会发生什么？它的回答质量会如何？我们又该如何评估和优化它的表现？

带着这些思考，准备好在下一节的挑战中，真正地与一个大语言模型进行"对话"了吗？
