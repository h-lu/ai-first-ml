# 16.1 Why: 为何需要“试错”与“探索”？

SFT教会了模型“模仿”，RM教会了模型“品味”。我们似乎已经拥有了一位既懂业务、又有品味的“AI客服”。那为什么我们还需要更复杂的PPO或DPO呢？

因为“模仿”和“品味”本身，并不能带来真正的“**创造**”和“**优化**”。

让我们回到“咖啡豆奇旅”的例子。通过SFT，我们的模型学会了客服手册里的标准回答。通过RM，我们有了一个可以判断回答好坏的裁判。

但想象一下这个场景：
*   **顾客提问：** “你们的‘奇旅拼配’，如果我想让它的口感更顺滑一点，冲煮的时候有什么技巧吗？”

这个问题非常具体，可能并未出现在我们SFT的“剧本”里。此时，SFT模型可能会：
1.  给出一个“安全”但无用的回答，因为它在剧本里找不到精确匹配。
2.  或者，它可能会尝试组合剧本里的不同知识，但组合出的回答可能并不理想。

**此时，我们训练好的“品味裁判”（RM模型）就派上了用场。**

假设SFT模型“探索”出了两个不同的新回答：
*   **探索A：** “您可以尝试降低水温冲煮。”
*   **探索B：** “您可以尝试将咖啡豆磨得更细一点，并适当缩短冲煮时间。”

现在，我们可以把这两个“探索性”的回答，拿给我们的“品味裁判”打分。裁判根据它学到的“品味”，可能会给“探索B”打一个更高的分数。

这个“**更高分**”的信号，就是一个极其重要的**奖励（Reward）**。它告诉我们的SFT模型：“嘿，你刚刚的第二个探索非常棒！这是一个正确的进化方向，请多尝试像这样的回答！”

这就是PPO和DPO这类基于强化学习的对齐算法的核心价值：**它们为模型提供了一个“试错”和“探索”的框架，并通过“奖励”信号，来引导模型朝着生成更优回答的方向去“进化”，而不是仅仅满足于模仿已有的标准答案。**

*   **SFT** 的目标是：**最小化**与标准答案的**差距**。
*   **PPO/DPO** 的目标是：**最大化**从奖励模型中获得的**分数**。

这个从“最小化差距”到“最大化分数”的转变，是AI从一个“模仿者”进化为“创造者”的关键一步。它使得模型有能力去发现那些我们人类自己都未曾想到的、但却能更好地满足我们偏好的、更优秀的解决方案。
