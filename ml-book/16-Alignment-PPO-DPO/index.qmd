# 第16章 第三步：对齐进化 —— 从经典PPO到现代DPO


欢迎来到对齐工程的"健身房"。

在之前的章节中，我们已经成功地完成了两项至关重要的准备工作：

1.  **训练了一位"演员" (SFT模型):** 这位演员已经通过"岗前培训"，熟读了我们的"剧本"，掌握了"咖啡豆奇旅"的专业知识和基本沟通风格。
2.  **培养了一位"裁判" (奖励模型):** 这位裁判通过学习我们的偏好，已经形成了相当不错的"品味"，能够判断出什么样的回答是"好"的，什么样的回答是"平庸"的。

现在，万事俱备，只欠东风。是时候让我们的"演员"真正地"动起来"，在"裁判"的实时指导下，通过不断的"**试错**"和"**探索**"，去发现那些我们从未明确写在剧本里、但却能获得裁判更高"品味分"的、更优秀的回答方式。

这个让AI自我探索、自我进化的过程，就是强化学习（Reinforcement Learning）在对齐工程中的核心应用。

在本章，我们将探索两条通往"更高智能"的山顶路径：

*   **经典之路 (PPO):** 我们将学习经典的**近端策略优化（Proximal Policy Optimization, PPO）**算法。它就像一个组织严密的电影拍摄现场，演员、裁判、导演（PPO算法）都在场，实时互动，不断打磨每一个镜头。这个方法非常强大，是很多里程碑式模型（如ChatGPT早期版本）的基石。
*   **现代捷径 (DPO):** 我们也将学习更现代、更高效的**直接偏好优化（Direct Preference Optimization, DPO）**。它另辟蹊径，跳过了显式训练"裁判"的环节，找到了一条更直接、更稳定的道路来学习人类偏好。

通过对比这两条路径，你将对现代LLM的对齐技术，形成一个完整而深刻的理解。准备好，见证你的AI实现真正的"进化"了吗？ 