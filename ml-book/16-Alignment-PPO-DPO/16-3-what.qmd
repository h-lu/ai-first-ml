# 16.2 What: 两条进化之路 PPO vs. DPO

::: {.callout-tip title="核心概念：PPO vs. DPO"}

我们已经明确，需要通过"最大化奖励分数"来驱动AI进化。现在，我们来认识一下实现这一目标的两种主流技术：**PPO（近端策略优化）** 和 **DPO（直接偏好优化）**。

---

#### **经典之路：PPO (Proximal Policy Optimization)**

想象一个极其专业的**电影拍摄现场**，这里有三个关键角色：

1.  **演员 (Actor):** 我们的SFT模型，它负责根据剧本（用户问题）"表演"出台词（生成回答）。
2.  **裁判 (Critic/Reward Model):** 我们的RM模型，它在现场实时观看演员的每一句台词，并立即给出一个"品味分"（奖励）。
3.  **导演 (PPO Algorithm):** PPO算法本身。导演的目标是让最终的电影（AI的行为）获得尽可能高的评分。

**PPO的工作流程就像这样：**
*   **开拍 (Generation):** 导演让演员针对一个场景（问题）即兴表演一句台词（回答）。
*   **实时打分 (Reward):** 裁判立刻对这句台词打分。
*   **导演指导 (Optimization):** 导演根据裁判的分数，对演员进行"指导"。如果分数高，导演会鼓励演员："很好，保持这个感觉！"如果分数低，导演会说："不对，我们换一种方式试试。" PPO的精髓在于，它的"指导"非常温和，它会告诉演员"在你原有风格的基础上，稍微往高分的方向调整一点点"，而不是让他完全推翻重来，这保证了训练的稳定性。

这个"**表演 -> 打分 -> 指导**"的循环不断重复，演员的演技（模型的能力）就在这个过程中持续提升。

**优点：** 效果强大，理论成熟，是许多里程碑式模型的基石。
**缺点：** 流程复杂，需要同时维护和调用多个模型（演员、裁判、导演），像一个庞大的摄制组，计算开销大，训练不稳定。

---

#### **现代捷径：DPO (Direct Preference Optimization)**

现在，想象一位更"现代"的导演，他找到了一种更高效的工作方式。他不再需要一个庞大的摄制组和一位在现场实时打分的裁判。

这位导演拿到的是一本特殊的"**批注剧本**"。

*   **批注剧本 (Preference Dataset):** 这就是我们之前用过的偏好数据集。剧本的每一页上，都写着同一个场景（问题）的**两种不同台词**（`chosen`回答 和 `rejected`回答），并且已经明确批注了"**这句更好**"。

**DPO的工作流程极其简洁：**
*   **读剧本 (Training):** DPO算法直接让演员（SFT模型）阅读这本"批注剧本"。
*   **领悟偏好 (Implicit Reward):** 演员在阅读时，会自己进行比较和领悟："哦，原来导演（人类）喜欢第一种台词，不喜欢第二种。我明白了。"
*   **自我修正 (Optimization):** 演员直接根据从"批注"中领悟到的"偏好"，来调整自己的表演风格。DPO通过一个巧妙的损失函数，将"偏好"直接转化为了对模型参数的更新。

DPO的革命性在于，它证明了**我们不需要一个明确的"裁判"来打分**，模型可以直接从成对的"好/坏"范例中，**隐式地**学到奖励，并完成自我优化。

**优点：** 流程极其简单，不需要独立的奖励模型，训练过程更稳定，计算开销小得多。
**缺点：** 是一个较新的研究方向，但在实践中已被证明非常有效，并迅速成为业界主流。

**一句话总结：PPO是通过"最大化一个明确的奖励分数"来学习，而DPO是通过"最大化满足人类偏好的概率"来学习。** 两条路都能通向山顶，但DPO为我们提供了一条更平坦、更宽阔的"高速公路"。
::: 