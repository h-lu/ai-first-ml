---
title: "16.2 How: 如何驱动AI自我进化？"
---

我们已经明确，要让AI超越"模仿"，实现"创造"，关键在于为其建立一个"试错和探索"的框架。但具体如何操作呢？一个漫无目的的AI，只会胡乱探索，我们该如何引导它的探索方向呢？

让我们带着这个问题，开启与AI的探索之旅。

::: {.callout-note title="与AI的探索之旅"}

**你：** 你好AI。我现在有两个模型：一个SFT模型（演员），它能生成不错的回答；一个RM模型（裁判），它能给任何回答打一个"品味分"。我如何利用这个"裁判"，来提升"演员"的演技呢？

**AI助手：** 绝佳的组合！这正是强化学习（RL）发挥作用的经典场景。我们可以搭建一个系统，让"演员"不断地生成新回答，然后让"裁判"实时地给这些回答打分。

**你：** 我明白了。但然后呢？"演员"看到了分数，它如何根据这个分数来"改进"自己？它的大脑（模型参数）要如何更新？

**AI助手：** 这就是整个流程最核心的一步。我们可以把这个过程想象成一个游戏循环：
1.  **探索 (Explore):** "演员"在当前剧本（问题）下，稍微改变一下表演方式，说一句与SFT标准答案略有不同的新台词（回答）。
2.  **获取奖励 (Reward):** "裁判"看到这句新台词后，给出一个分数。这个分数，就是AI在这次探索中获得的**奖励**。
3.  **学习 (Learn):** 如果奖励分数很高，"演员"的大脑中就会有一个机制告诉它："刚才的尝试非常成功！我要调整我的表演策略，增加以后说出类似台词的概率。"反之，如果奖励分数很低，这个机制就会说："这次尝试很失败，我要减少说出类似台词的概率。"

**你：** "增加或减少说出类似台词的概率"……这听起来很专业。在技术上，这个"学习机制"是如何实现的？

**AI助手：** 非常好的问题。这个"学习机制"就是强化学习算法的核心，例如PPO。它的目标函数被设计为**最大化预期的累积奖励**。简单来说，它会通过复杂的数学计算（主要是策略梯度），调整SFEL模型（演员）的参数，使得它在未来生成回答时，更有可能生成那些能从RM模型（裁判）那里获得高分的回答。

**你：** 我好像有点理解了。所以，整个流程就像是我在训练一只小狗。我让它"握手"，它随机做了一个动作，如果做对了，我就给它一块零食（高奖励），它就学会了以后更多地做这个动作。这里的PPO/DPO算法，就扮演了那个根据"零食"来更新小狗大脑连接方式的角色。

**AI助手：** 这个类比非常生动和准确！你已经抓住了RLHF（基于人类反馈的强化学习）的本质。我们的核心任务，就是建立起这个"**生成 -> 打分 -> 学习**"的正向反馈飞轮。一旦飞轮转起来，AI就会在"最大化奖励"这个单一目标的驱动下，开始它的自我进化之旅。
::: 