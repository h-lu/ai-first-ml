# 13.1 Why: "我的机器人有时还是会胡说八道！"

我们满怀期待地向我们新生的RAG机器人提出了第一个真正的问题：

**👤 你:**
> "在RAG系统中，Chunking策略对最终结果有什么影响？"

我们期望它能根据我们提供的知识库，给出一个精准、可靠的答案。但它却回答：

**🤖 RAG机器人 (v1.0):**
> "RAG系统中的Chunking策略，类似于数据库的分片技术，主要目的是为了提升查询速度和负载均衡..."

这个答案听起来似乎有些道理，但如果你仔细核对我们给它的"学习材料"（知识库），你会发现材料里根本没有提到"数据库分片"或"负载均衡"。机器人开始**一本正经地胡说八道**了——这就是典型的**"幻觉" (Hallucination)**。

---

或者，我们遇到了另一种情况。我们的知识库里明确有一段话是："文本分块（Chunking）的最佳大小，取决于具体任务和文档的结构。较小的块能提高检索精度，但可能导致上下文不完整；较大的块能提供更完整的上下文，但可能引入噪声。"

我们提问：
**👤 你:**
> "RAG的Chunk Size是不是越大越好？"

**🤖 RAG机器人 (v1.0):**
> "根据我现有的资料，无法回答这个问题。"

明明知识库里有相关信息，机器人却未能有效利用，给出了一个**过于保守、不够智能**的回答。

---

### 问题的根源：检索质量是RAG的基石

这两个"Bad Cases"指向了同一个核心问题：**检索阶段的输出质量，直接决定了生成阶段的答案质量。**

> "Garbage in, garbage out." (垃圾进，垃圾出。)

对于RAG系统来说，这个古老的计算机科学谚语再贴切不过了。LLM就像一个技艺高超的厨师，但如果你给他的"食材"（检索到的`context`）本身就是错误的、不完整的、或是包含太多无关信息的，那么无论他的"厨艺"（生成能力）多么高超，也无法烹饪出一道完美的菜肴。

-   在第一个例子中，我们的检索系统可能"捞"上来了一些不那么相关的文档块，这些文档块恰好包含了一些与"系统"、"策略"相关的词，导致LLM在生成时错误地联想到了"数据库分片"。
-   在第二个例子中，检索系统可能只"捞"到了一半相关的句子，未能形成完整的逻辑链，导致LL-M觉得信息不足，不敢作答。

因此，想要让我们的机器人从"有时幻觉"进化到"忠于事实"，我们必须将优化的目光聚焦在RAG流程的上游——**提升我们的信息检索质量**。

在下一节，我们将与AI一起，对这些"Bad Cases"进行深入分析，并探讨几种立竿见影的优化策略。
