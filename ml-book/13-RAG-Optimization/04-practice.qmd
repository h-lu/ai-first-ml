## 13.4 Practice: 设计你的第一个RAG优化实验

理论已经掌握，现在是时候像一个真正的数据科学家一样，通过**设计和执行实验**，来科学地优化我们的RAG系统了。

我们不会给你一段可以直接复制粘贴的最终代码，因为优化的过程本身就是学习的一部分。相反，我们将为你提供一份详尽的"指令剧本"，指导你如何指挥你的AI编程助手，一步步地搭建一个用于评估不同参数组合的实验框架。

---

### 实验目标

我们的目标是：**量化地评估不同 `chunk_size` 和 `top_k` 组合对RAG系统回答质量的影响。**

### 实验设计

一个经典的实验设计包含以下要素：
1.  **自变量 (Independent Variables)**: 我们要调整的参数。这里是 `chunk_size` 和 `top_k`。
2.  **因变量 (Dependent Variable)**: 我们要观察的指标。这里是"答案的质量"，我们需要一个方法来评估它。
3.  **控制变量 (Controlled Variables)**: 保持不变的参数，以确保公平比较。这里包括 `embedding_model`, `prompt_template` 等。

---

::: {.callout-note title="RAG优化实验指令剧本" icon="fas-flask"}

**👤 你的指令 (第一部分：搭建实验框架):**

> 你好，AI助手。我需要你的帮助来设计一个评估RAG系统性能的实验框架。请帮我编写一个Python脚本，完成以下准备工作：
>
> 1.  **建立一个"黄金标准"测试集**:
>     -   创建一个名为 `eval_dataset` 的列表。
>     -   列表中的每个元素都是一个字典，包含两个键：`question` (一个你认为系统应该能回答好的问题) 和 `ground_truth` (这个问题的标准答案，由你手动编写)。
>     -   请帮我预置3-5个高质量的问答对。例如，问题可以是"RAG的核心思想是什么？"，标准答案可以是"RAG的核心思想是在生成答案前，先从外部知识库中检索相关信息，以减少幻觉并提高事实准确性。"
>
> 2.  **封装RAG流程**:
>     -   将我们上一章的 `answer_question` 函数，以及其依赖的 `embedding_model`, `faiss` 等组件，都封装到一个名为 `RAGPipeline` 的类中。
>     -   这个类的 `__init__` 方法应该接收 `documents` 作为输入，并在内部完成文本分块（chunking）和FAISS索引的构建。让 `chunk_size` 和 `chunk_overlap` 也成为 `__init__` 的参数。
>     -   这个类需要有一个 `answer` 方法，它接收 `question` 和 `top_k` 作为参数，执行检索和生成Prompt的完整流程，并返回最终的（模拟）LLM回答（即包含上下文和问题的Prompt）。
>
> 3.  **设计评估函数 (初步)**:
>     -   创建一个名为 `evaluate_answer` 的函数，它接收三个参数：`generated_answer` (我们的RAG系统生成的答案), `ground_truth` (我们的标准答案), 和 `question`。
>     -   **在函数内部，我们需要一种方法来评估 `generated_answer` 的质量。这是一个开放性问题，请你向我建议至少两种评估方法，并解释它们的优缺点。** (例如：使用Embedding相似度，或使用更高级的LLM-as-a-judge方法)。

---

**🤖 AI的预期回答 (关于评估方法):**

> ... (AI应该会生成上述框架代码) ...
>
> 对于 `evaluate_answer` 函数，这里有两种评估生成答案质量的常见方法：
>
> **方法1: 基于嵌入的相似度 (Embedding-based Similarity)**
> *   **做法**: 分别计算 `generated_answer` 和 `ground_truth` 的句嵌入向量，然后计算它们之间的余弦相似度。分数越高，代表语义上越接近。
> *   **优点**: 计算简单、快速、成本低。
> *   **缺点**: "词义相近"不完全等同于"事实正确"。两个句子可能用了相似的词，但表达了完全相反的事实。它只能作为初步的、粗略的评估。
>
> **方法2: LLM作为评判者 (LLM-as-a-Judge)**
> *   **做法**: 设计一个新的Prompt，将 `question`, `ground_truth`, 和 `generated_answer` 同时提供给一个强大的LLM（如GPT-4），然后要求它根据"事实一致性"、"完整性"等标准，给出一个1-5分的评分。
> *   **优点**: 评估质量非常高，能理解细微的语义差别和事实错误，更接近人类的判断。
> *   **缺点**: 成本高（需要调用强大的LLM API），速度慢，并且评估结果本身也可能存在少量不稳定性。
>
> **建议**: 在这个实验中，我们可以先从**方法1**入手，因为它更容易实现。

---

**👤 你的指令 (第二部分：运行实验循环):**

> 好的，我们采纳基于嵌入相似度的评估方法。现在请帮我完成实验的执行部分：
>
> 1.  **定义参数网格**:
>     -   创建一个 `param_grid` 字典，其中定义我们要测试的参数组合。例如:
>         ```python
>         param_grid = {
>             "chunk_size": [256, 512],
>             "chunk_overlap": [20, 50],
>             "top_k": [3, 5]
>         }
>         ```
> 2.  **编写实验主循环**:
>     -   遍历 `param_grid` 中的所有参数组合。
>     -   在每次循环中：
>         a.  根据当前的 `chunk_size` 和 `chunk_overlap`，实例化一个新的 `RAGPipeline`。
>         b.  初始化一个用于记录本次参数组合平均分的变量 `total_score`。
>         c.  遍历 `eval_dataset` (我们的"黄金标准"测试集)。
>         d.  对于测试集中的每一个问答对，使用当前的 `top_k` 调用 `pipeline.answer()` 方法，生成答案。
>         e.  调用我们实现的 `evaluate_answer` 函数，计算生成答案与标准答案的相似度得分。
>         f.  累加得分到 `total_score`。
>     -   计算本次参数组合的平均分，并打印出来，格式如下: `参数: {'chunk_size': 256, ...}, 平均分: 0.85`
> 3.  **找出最佳参数**:
>     -   在所有循环结束后，打印出得分最高的那组参数组合。
>
> 请将以上逻辑整合到我们的脚本中。

:::

---

通过与AI协同完成这个指令剧本，你将获得一个虽小但五脏俱全的RAG评估框架。这个框架不仅能帮助你找到当前项目的最佳参数，更重要的是，它为你提供了一套未来可以不断扩展和复用的、科学评估和迭代AI系统的方法论。

</rewritten_file>