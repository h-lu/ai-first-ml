# 17.1 Why: 当“标准答案”不再唯一的挑战

在本书的前两个部分，我们已经熟练掌握了一套评估模型的“标尺”：准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F1分数... 它们都非常有效，因为它们都基于一个共同的前提：**存在一个唯一的、正确的“标准答案”**。

AIGC内容的“有害/无害”分类是明确的，模型的预测结果要么对，要么错。

但是，当我们进入生成式AI的领域，这个前提开始动摇了。

让我们回到“咖啡豆奇旅”的场景。假设我们问了模型一个问题：

> **你：“请向我推荐一款适合早晨喝的咖啡豆。”**

我们训练好的两个模型，SFT模型和DPO模型，可能给出了两个不同的回答：

*   **模型A (SFT):** “我们推荐‘晨曦之光’拼配。它由埃塞俄比亚和哥伦比亚的咖啡豆混合而成，带有柑橘和花香，口感明亮，非常适合开启新的一天。” (基于SFT数据中的描述)

*   **模型B (DPO):** “如果你想在清晨唤醒活力，我强烈推荐‘晨曦之光’！想象一下，那清新的柠檬香气和淡淡的茉莉花香在你的舌尖跳跃，是不是很棒？它的酸度恰到好处，能让你立刻精神焕发。” (经过DPO优化，更具“个性”和“感染力”)

哪一个回答是“正确”的？

答案是：**它们可能都是正确的，或者说，都是“好”的回答**。模型A的回答更像一个产品说明书，准确、客观。模型B的回答则更像一个热情的咖啡师，生动、有感染力。

我们无法用简单的“对/错”来评判。更糟糕的是，传统的评估指标在这里完全派不上用场。我们总不能去计算模型B的回答和某个“标准文案”之间的F1分数吧？

这就是我们在LLM时代遇到的核心评估困境：**当评价一个模型的好坏从“是否正确”转变为“是否优秀”、“是否有用”、“是否更符合人类偏好”时，我们需要一套全新的评估范式。**

这套范式不再追求与标准答案的像素级匹配，而是要能在一个“开放式”的问题空间里，公平地比较出哪个模型是“更好的那个”。 