# 7.5 Challenge & Toolbox

## 第一部分：动手挑战 - 成为'炼丹'艺术家——初探超参数调优

在上一节，我们见证了LightGBM的强大威力，它的性能远超逻辑回归。但我们所用的，只是一个"开箱即用"的、使用默认参数的LightGBM。

在机器学习领域，提升模型性能的最后（也往往是最有效）一公里，常常来自于对模型**超参数（Hyperparameters）**的精细调整。这个过程充满了实验和探索，因此被工程师们戏称为"炼丹"。

你的挑战是：在AI的指导下，扮演一回"炼丹师"，对我们的LightGBM模型进行你的第一次超参数调优。

#### 任务1：AI，什么是"炼丹炉"的"旋钮"？

在"炼丹"之前，你必须先理解"炼丹炉"上那些复杂旋钮的含义。

::: {.callout-note title="你的指令剧本" icon="fas fa-question-circle"}
我正在使用LightGBM模型，并准备对它进行超参数调优。我注意到了这三个参数：`n_estimators`，`learning_rate` 和 `num_leaves`。

请你用一个生动的比喻（比如把模型训练比作"学生学习"），向我解释这三个超参数分别控制了学习过程的哪个方面？它们调得太高或太低，分别会有什么效果或风险（比如"学得太慢"或"死记硬背"）？
:::

理解了这些，你才能做出有根据的调整，而不是盲目尝试。

#### 任务2：动手"调参"，观察火焰的变化

现在，让我们亲手拧动一个"旋钮"，看看"火焰"会发生什么变化。我们将从 `num_leaves` 开始，它控制了模型能学习到的"规则"的复杂度。

::: {.callout-note title="你的指令剧本" icon="fas fa-tools"}
感谢你的解释！我现在想动手实验一下 `num_leaves` 这个超参数。

请帮我编写一段Python代码，完成以下任务：

1.  创建一个 `num_leaves` 的候选值列表，例如 `[10, 20, 31, 40, 50]`。
2.  编写一个`for`循环，遍历这个列表中的每一个值。
3.  在循环内部，创建、训练一个新的LightGBM模型，并将当前的候选值赋给 `num_leaves` 参数。
4.  在测试集上评估该模型，并打印出当前的 `num_leaves` 值和它对应的F1分数。

我想通过这个实验，找到在当前任务中，`num_leaves` 的最佳取值范围。
:::

#### 任务3：思辨：调参是"万能灵药"吗？

调参非常强大，但也容易让人陷入一个误区：盲目追求分数的提升，而忽略了其背后的代价和风险。

::: {.callout-note title="与AI进行一场思辨对话" icon="fas fa-brain"}
我发现通过调优超参数，确实可以提升模型的F1分数。这让我很兴奋，但也有一些疑问。请和我探讨一下：

1.  是不是超参数调优的过程越复杂、搜索的候选值越多，最终得到的模型就一定越好？
2.  这个过程有没有可能带来什么负面效果？比如，我听说过一个词叫"过拟合到验证集上"，这是什么意思？它在我们的调参过程中是如何发生的？
3.  除了提升模型分数，超参数调优还有没有其他我们应该关注的目标？（比如模型的训练速度、预测速度等）
:::

这个思辨将让你对模型优化有一个更成熟、更全面的认识，而不仅仅是盯着评估指标。

---

## 第二部分：AI协同工具箱 - 系统化你的模型实验

### 从"一次性脚本"到"可追溯的科学实验"

在上一节，我们基于分析结果，提出了多个模型迭代的假设：
-   尝试新算法，如LightGBM。
-   调整模型参数。
-   使用不同的特征工程方法。
-   处理类别不平衡问题。

很快，你就会发现自己陷入了一个新的困境：
-   你尝试了10个不同的模型，哪个效果最好来着？
-   模型A在"有害"类别上召回率高，但模型B在"低质"类别上精确率高，如何取舍？
-   我三个月前做的那个效果不错的实验，参数到底是怎么设置的？

如果你的所有实验都只是一些散乱的Jupyter Notebook或Python脚本，那么你的项目很快就会变得混乱不堪、无法管理。

科学的进步依赖于**可复现的实验**。机器学习作为一门实验科学，同样如此。我们需要一个工具来系统地管理我们的实验过程，这就是**实验跟踪（Experiment Tracking）**。

### 什么是实验跟踪？

实验跟踪就是将你每一次模型训练的"四件套"系统地记录下来的过程：
1.  **代码版本 (Code Version)**：你用了哪个版本的代码？(例如，Git commit hash)
2.  **输入数据 (Input Data)**：你用了哪个版本的数据集？
3.  **超参数 (Hyperparameters)**：模型的参数配置是什么？(例如，`LogisticRegression(C=0.5, solver='saga')`)
4.  **输出结果 (Output)**：模型的性能指标（如F1分数）和产出的模型文件是什么？

像 [MLflow](https://mlflow.org/) 和 [Weights & Biases](https://wandb.ai/) 这样的工具，就是专门为此设计的。它们能帮你创建一个"实验日志"，让你所有的尝试都一目了然，方便你比较、复现和分享。

### 与AI协同，快速搭建一个基础实验跟踪器

虽然在入门阶段我们不一定需要引入一个重型的实验跟踪框架，但我们可以利用AI，快速为我们的代码增加基础的实验跟踪功能。让我们来指挥AI完成这个任务。

::: {.callout-note title="AI指令模板：为训练脚本添加实验跟踪功能" icon="fas fa-cogs"}
**# 角色**
你是一位注重代码规范和可复现性的资深机器学习工程师。

**# 上下文**
我现在有一个模型训练的脚本。它能够训练一个模型并评估其性能。但每次运行，结果都会被覆盖，无法比较不同实验。

**# 任务**
请帮我重构这段代码，加入一个基础的实验跟踪功能。我希望实现以下目标：
1.  **创建一个`run_experiment`函数**：将单次模型训练和评估的逻辑封装在这个函数里。
2.  **记录实验结果**：函数应该返回一个包含关键信息的字典，例如：`model_name`, `accuracy`, `macro_f1_score`，以及详细的 `classification_report`。
3.  **系统化运行和比较**：
    *   创建一个列表`results_list`来保存每次实验的结果。
    *   在一个主流程中，我可以定义不同的模型（比如逻辑回归和LightGBM），然后循环调用`run_experiment`函数。
    *   最后，将`results_list`转换为一个Pandas DataFrame，并打印出来，这样我就可以清晰地比较不同实验的结果。
4.  **代码模块化**：请将数据加载和预处理的逻辑也封装成独立的函数，让主脚本更清晰。

**# 原始代码片段 (示意)**
```python
# 数据加载...
# 特征工程...
# 数据切分...

# 模型1：逻辑回归
lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print(classification_report(y_test, y_pred_lr))

# 模型2：LightGBM (可能之后会加)
# lgb = lgb.LGBMClassifier()
# lgb.fit(X_train, y_train)
# y_pred_lgb = lgb.predict(X_test)
# print(classification_report(y_test, y_pred_lgb))
```

**# 输出格式**
提供一个完整的、重构后的、可以直接运行的Python脚本。
:::

---

#### AI重构后的代码 (预期输出)

一个好的AI助手会返回一个结构清晰、模块化的脚本，如下所示：

```python
#| eval: false
#| code-fold: true
#| code-summary: "点击查看AI重构后的完整实验跟踪脚本"
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score
from sklearn.preprocessing import LabelEncoder

# --- 1. 数据处理模块 ---
def load_and_preprocess_data(filepath):
    """加载数据，处理缺失值，并进行标签编码。"""
    data = pd.read_csv(filepath)
    data['content'] = data['content'].fillna('')
    
    label_encoder = LabelEncoder()
    data['label_encoded'] = label_encoder.fit_transform(data['label'])
    
    return data, label_encoder

def create_features(data):
    """使用TF-IDF创建文本特征。"""
    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(data['content'])
    return tfidf_matrix, vectorizer

# --- 2. 实验运行模块 ---
def run_experiment(model, X_train, y_train, X_test, y_test, label_encoder):
    """训练一个模型并返回其性能指标。"""
    model_name = model.__class__.__name__
    print(f"--- Running experiment for: {model_name} ---")
    
    # 训练
    model.fit(X_train, y_train)
    
    # 预测
    y_pred = model.predict(X_test)
    
    # 评估
    accuracy = accuracy_score(y_test, y_pred)
    macro_f1 = f1_score(y_test, y_pred, average='macro')
    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_, output_dict=True)
    
    # 记录结果
    result = {
        'model_name': model_name,
        'accuracy': accuracy,
        'macro_f1_score': macro_f1,
        'classification_report': report
    }
    
    print(f"--- Experiment for {model_name} finished. Macro F1: {macro_f1:.4f} ---\n")
    return result

# --- 3. 主流程 ---
if __name__ == '__main__':
    # 加载和准备数据
    # 注意：请将 'aigc_content.csv' 替换为你的数据文件路径
    data, label_encoder = load_and_preprocess_data('aigc_content.csv')
    tfidf_matrix, vectorizer = create_features(data)
    
    # 切分数据集
    X_train, X_test, y_train, y_test = train_test_split(
        tfidf_matrix, 
        data['label_encoded'], 
        test_size=0.2, 
        random_state=42, 
        stratify=data['label_encoded']
    )
    
    # 定义要尝试的模型
    models_to_try = [
        LogisticRegression(max_iter=1000, random_state=42),
        LGBMClassifier(random_state=42)
    ]
    
    # 运行所有实验并收集结果
    results_list = []
    for model in models_to_try:
        exp_result = run_experiment(model, X_train, y_train, X_test, y_test, label_encoder)
        results_list.append(exp_result)
        
    # 将结果转换为DataFrame以便于比较
    results_df = pd.DataFrame(results_list)
    
    # 提取每个类别的F1分数，方便比较
    for label in label_encoder.classes_:
        results_df[f'f1_{label}'] = results_df['classification_report'].apply(lambda x: x[label]['f1-score'])
        
    # 打印最终的比较表格 (可以按某个关键指标排序)
    print("--- Experiment Comparison Summary ---")
    print(results_df[['model_name', 'accuracy', 'macro_f1_score', 'f1_有害', 'f1_低质', 'f1_优质']].sort_values(by='macro_f1_score', ascending=False))

```
通过这种方式，你可以轻松地在`models_to_try`列表中添加更多模型或不同参数的同一模型，脚本会自动运行、评估并生成一个清晰的对比表格，让你的模型迭代过程变得前所未有的系统和高效。

::: 