# 7.4 Practice: 指挥AI生成并解读多维度评估报告

::: {.callout-caution title="依赖项安装" icon="fas fa-download"}
本次实践除了需要 `scikit-learn` 和 `seaborn` 外，还需要安装一个强大的梯度提升库 `lightgbm`。如果你的环境中尚未安装，请运行以下命令：

```bash
#| eval: false
pip install lightgbm
```
:::

## 第一步：生成专业评估报告

我们将使用`scikit-learn`中两个强大的工具：`confusion_matrix`和`classification_report`。

::: {.callout-note title="AI指令模板：生成并可视化评估报告" icon="fas fa-chart-bar"}
**# 角色**
你是一位精通`scikit-learn`和数据可视化（如`seaborn`）的Python数据科学家。

**# 上下文**
我已经有了一个训练好的分类模型`model`，以及测试集的真实标签`y_test`和预测标签`y_pred`。我还保留了用于解码标签的`label_encoder`，它知道数字（0,1,2）和真实类别（"有害", "低质", "优质"）的对应关系。

**# 任务**
请帮我编写一段Python代码，完成以下任务：
1.  **生成分类报告**:
    *   从`sklearn.metrics`导入`classification_report`。
    *   调用该函数，传入`y_test`和`y_pred`。
    *   为了报告的可读性，请使用`target_names=label_encoder.classes_`来显示真实的类别名称，而不是0,1,2。
    *   将生成的报告打印出来。

2.  **生成并可视化混淆矩阵**:
    *   从`sklearn.metrics`导入`confusion_matrix`。
    *   调用该函数，生成混淆矩阵。
    *   使用`seaborn.heatmap()`来创建一个美观的热力图，以可视化混淆矩阵。
    *   在热力图上，请：
        *   使用`annot=True`和`fmt='d'`来在每个格子里显示具体的数字。
        *   使用`cmap='Blues'`颜色主题。
        *   设置x轴和y轴的标签，使用`label_encoder.classes_`来显示类别名称，并分别命名为"Predicted Label"和"True Label"。
        *   添加一个清晰的标题，例如"Confusion Matrix for AIGC Content Classification"。

**# 输出格式**
请提供可以直接运行的、结构清晰的Python代码，并为关键步骤添加注释。
:::

## 第二步：解读与分析

在运行AI生成的代码后，你将得到两份关键的输出：一份文本报告和一张热力图。

#### 示例输出 1: 分类报告 (Classification Report)

```
                     precision    recall  f1-score   support

           有害       0.75      0.60      0.67        10
           低质       0.85      0.88      0.86       140
           优质       0.98      0.99      0.98       850

      accuracy                           0.95      1000
     macro avg       0.86      0.82      0.84      1000
  weighted avg       0.95      0.95      0.95      1000
```

**如何解读这份报告？**
-   **逐行看 (按类别)**:
    -   **有害 (Harmful)**:
        -   `precision`=0.75: 在所有被模型标记为"有害"的内容中，75%是真有害，25%是误杀。
        -   `recall`=0.60: 在所有真正的"有害"内容中，模型只成功找出了60%，有40%的"漏网之鱼"！**这是一个巨大的警报！**
        -   `f1-score`=0.67: 综合分数不高，主要是被低召回率拖累了。
    -   **低质 (Low Quality)**: 各项指标在85%左右，表现尚可。
    -   **优质 (High Quality)**: 各项指标都接近99%，表现非常好。这不奇怪，因为它的样本量最大。
-   **看平均值 (宏观)**:
    -   `accuracy`=0.95: 这就是我们之前看到的、具有欺骗性的总体准确率。
    -   `macro avg` (宏平均):
        -   它的F1分数是0.84，比加权平均的0.95低很多。这是因为**宏平均平等地看待每个类别**，"有害"类别的糟糕表现严重拉低了平均分。**在类别不平衡时，我们应该更关注宏平均！**
    -   `weighted avg` (加权平均):
        -   它的F1分数是0.95，和准确率很接近。因为它按样本量加权，"优质"类别的高分主导了结果。

#### 示例输出 2: 混淆矩阵热力图

![Confusion Matrix Heatmap](https://i.imgur.com/example.png)  (这是一个示意图，真实的热力图会由代码生成)
```
          Predicted: 有害  Predicted: 低质  Predicted: 优质
Actual: 有害         6             3             1
Actual: 低质         4            123           13
Actual: 优质         1             8            841
```

**如何解读这张图？**
-   **看对角线 (TP)**: `(6, 123, 841)` 是模型预测正确的数量。我们希望这些数字越大越好。
-   **看非对角线 (Errors)**: 这些是模型犯错的地方。
    -   **第一行 (Actual: 有害)**:
        -   总共有 `6+3+1=10` 个真实有害样本。
        -   模型正确识别了6个 (TP)。
        -   **模型将3个有害内容错判为"低质" (FN)**。
        -   **模型将1个有害内容错判为"优质" (FN)**。 这两种是**最严重的漏报错误**！
    -   **第一列 (Predicted: 有害)**:
        -   总共有 `6+4+1=11` 个内容被预测为有害。
        -   其中6个是真有害 (TP)。
        -   **模型将4个低质内容错判为"有害" (FP)**。
        -   **模型将1个优质内容错判为"有害" (FP)**。 这两种是**误杀错误**。

## 第三步：训练并评估LightGBM模型

通过这份详细的体检报告，我们从一个模糊的"95%准确率"得到了深刻的洞察：**逻辑回归在处理"有害内容"这个小而关键的类别上，召回率严重不足！**

这引出了我们最重要的迭代假设：
> 逻辑回归是一个简单的线性模型，可能无法捕捉"有害"内容复杂的语义模式。我们应该尝试一个更强大的非线性模型，比如**梯度提升机(LightGBM)**。

现在，让我们立即将这个假设付诸实践！

::: {.callout-note title="AI指令模板：训练、评估并对比LightGBM模型" icon="fas fa-rocket"}
**# 角色**
你是一位熟悉`lightgbm`库和`scikit-learn`评估流程的专家。

**# 上下文**
我们已经对逻辑回归模型进行了评估，发现它在"有害"类别上的召回率很低。现在我们想尝试一个更强大的LightGBM模型来解决这个问题。我们拥有相同的训练集`X_train`, `y_train`和测试集`X_test`, `y_test`，以及`label_encoder`。

**# 任务**
请帮我编写一段Python代码，完成以下连贯的任务：
1.  **训练模型**：
    *   从`lightgbm`导入`LGBMClassifier`。
    *   初始化一个`LGBMClassifier`模型，设置`random_state=42`。
    *   在`X_train`和`y_train`上训练该模型。
2.  **进行预测**：
    *   使用训练好的LGBM模型，对`X_test`进行预测，结果保存在`y_pred_lgbm`中。
3.  **评估新模型**：
    *   复用我们之前为逻辑回归编写的评估代码，为LightGBM模型也生成一份完整的**分类报告**和**混淆矩阵热力图**。
    *   请确保报告和图表的标题能够清晰地区分这是LightGBM模型的结果（例如，标题可以包含"LightGBM"字样）。

**# 输出格式**
请提供可以直接运行的、连贯的Python代码，让我可以一次性看到新模型的训练和评估结果。
:::

## 第四步：对比与结论

当你运行完新的指令后，你将得到第二份评估报告。现在，将两份报告并排放在一起，你会发现惊人的变化：

**你可能会看到（示例）：**
-   **逻辑回归在"有害"类别上的召回率：0.60**
-   **LightGBM在"有害"类别上的召回率：0.80** (显著提升！)

这个对比雄辩地证明了我们的假设：**更换一个更强大的模型，确实能更好地捕捉复杂模式，从而显著提升关键业务指标（召回率）！**

当然，你也可能会发现LightGBM的精确率略有下降，或者训练时间更长。这些都是模型迭代中需要权衡的因素。

## 本节小结

### 🎯 核心技能
1.  **生成报告**: 你学会了指挥AI使用`classification_report`和`confusion_matrix`来生成专业的模型评估报告。
2.  **解读报告**: 你掌握了如何从分类报告和混淆矩阵中，分析模型在每个类别上的具体表现，以及它主要犯了哪些类型的错误（漏报 vs. 误杀）。
3.  **迭代与对比**: 你亲身体验了基于评估结果提出假设，并用一个更强大的模型进行验证和对比的完整迭代循环。

### 🤔 为何重要
能够生成并专业地解读模型评估报告，是区分机器学习初学者和专业人士的关键分水岭。它标志着你从一个"模型使用者"转变为一个能够诊断、分析并持续优化模型的"模型医生"。

在下一节，我们将学习如何系统地管理这些模型迭代过程，确保我们的每一次尝试都有记录、可追溯、可比较。 