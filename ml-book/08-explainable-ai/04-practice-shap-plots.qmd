# 8.4 Practice: 指挥AI用SHAP绘制模型解释图

## 让黑箱开口说话

理论学习已经结束，是时候拿起SHAP这个强大的"扳手"，撬开我们模型（比如LightGBM）的黑箱了。在本节中，你将亲手生成局部和全局的解释图，并学会如何解读它们。

为了让我们的图表能够正确生成和复现，我们将把本节的所有操作（从数据准备、模型训练到SHAP分析）都集中在一个可执行的代码块中。

```{python}
#| label: fig-shap-plots
#| fig-cap: "SHAP可解释性分析图"
#| code-fold: true
#| code-summary: "点击查看生成本章所有图表的代码"
#| eval: false
# 注意：此代码块默认不执行以节省资源。
# 在实际渲染书籍时，请将 `#| eval: false` 修改为 `#| eval: true` 来生成图表。

import pandas as pd
import shap
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from lightgbm import LGBMClassifier
from sklearn.preprocessing import LabelEncoder

# --- 1. 数据加载与预处理 ---
# 假设我们有一个名为 'aigc_content_tagged.csv' 的文件
# 为了可复现性，我们创建一个虚拟的DataFrame
content_data = {
    'content': [
        "惊艳的创新设计，深度好文", "这篇文章逻辑清晰，观点独特", "推荐，非常有价值的分享",
        "快来免费领取，限时福利，错过不再有", "立即点击，领取你的专属优惠", "转发集赞，赢取大奖",
        "内部消息，稳赚不赔，轻松实现财富自由", "一夜暴富不是梦，跟着我操作", "独家内幕，绝密资料",
        "这篇文章写得很好，分析很到位", "不错的观点，学到了很多", "内容详实，值得一看",
        "这是什么垃圾内容，浪费时间", "不知所云，逻辑混乱", "完全是标题党，不要看",
        "这个项目前景广阔，建议长期持有", "风险与机遇并存，需要谨慎分析",
        "快来加入我们，一起实现财务自由", "不要犹豫，立即行动，名额有限",
        "警告：该产品未经证实，投资需谨慎"
    ],
    'label': [
        "优质", "优质", "优质",
        "低质", "低质", "低质",
        "有害", "有害", "有害",
        "优质", "优质", "优质",
        "低质", "低质", "低质",
        "优质", "优质",
        "有害", "有害",
        "有害"
    ]
}
data = pd.DataFrame(content_data)
data['content'] = data['content'].fillna('')
label_encoder = LabelEncoder()
data['label_encoded'] = label_encoder.fit_transform(data['label'])

# 类别映射: 0:低质, 1:优质, 2:有害
# 我们在后续分析中将主要关注"有害"类别，其编码为2

# --- 2. 特征工程与数据切分 ---
vectorizer = TfidfVectorizer(max_features=100)
tfidf_matrix = vectorizer.fit_transform(data['content'])
X_train, X_test, y_train, y_test = train_test_split(
    tfidf_matrix, data['label_encoded'], test_size=0.2, random_state=42, stratify=data['label_encoded']
)

# --- 3. 模型训练 ---
lgb_model = LGBMClassifier(random_state=42)
lgb_model.fit(X_train, y_train)

# --- 4. SHAP分析 ---
# 4.1 初始化解释器
explainer = shap.TreeExplainer(lgb_model, X_train)
# 4.2 计算SHAP值
shap_values = explainer.shap_values(X_test)

# 加载JS可视化库
shap.initjs()

# 4.3 局部解释：分析第一个测试样本
# | label: fig-shap-force
# | fig-cap: "单个测试样本的SHAP力图（预测目标：有害）"
sample_index = 0
# 我们关注"有害"类别, 它的编码是2
print(f"为'有害'(类别2)生成Force Plot:")
shap.force_plot(
    explainer.expected_value[2], 
    shap_values[2][sample_index,:], 
    X_test[sample_index,:],
    feature_names=vectorizer.get_feature_names_out()
)

# 4.4 全局解释：分析整个模型对"有害"类别的判断依据
# | label: fig-shap-summary
# | fig-cap: "针对'有害'类别的全局SHAP摘要图"
print(f"\n为'有害'(类别2)生成Summary Plot:")
shap.summary_plot(
    shap_values[2], 
    X_test,
    feature_names=vectorizer.get_feature_names_out()
)

```

---

### 解读局部力图 (Force Plot)

在上面的代码块执行后，首先会生成一张针对单个样本的力图（如 @fig-shap-force 所示）。这张图告诉我们，对于我们选择的这个具体样本，模型是如何做出决策的。

**如何解读 @fig-shap-force**:
-   **base value (基础值)**: `explainer.expected_value[2]`，代表了在不知道任何具体词汇信息时，模型对于"有害"这个类别的平均预测概率（在logit空间）。这是我们分析的起点。
-   **output value (输出值)**: 模型对这一个样本的最终预测分数。如果这个值高于基础值，说明模型倾向于认为它是"有害"的。
-   **红色箭头 (正贡献)**: 将预测分数**推高**的特征。这些词汇是让模型认为这篇文章更可能是"有害"的"罪魁祸首"。例如，如果样本中出现了"稳赚不赔"，它很可能会在这里显示为一个强力的红色箭头。
-   **蓝色箭头 (负贡献)**: 将预测分数**拉低**的特征。这些是让模型认为这篇文章更不像"有害"的"减罪证据"。例如，"分析"、"观点"这类词可能会显示为蓝色箭头。
-   **箭头长度**: 代表了贡献的大小。箭头越长，说明这个词的影响力越大。

通过 @fig-shap-force，你可以清晰地向他人解释："模型之所以将这篇文章判定为'有害'，主要是因为其中出现的'词A'和'词B'，尽管'词C'在一定程度上降低了风险，但不足以扭转最终结果。"

---

### 解读全局摘要图 (Summary Plot)

力图之后，代码会生成一张全局摘要图（如 @fig-shap-summary 所示），它聚合了测试集中所有样本的SHAP值，让我们能理解模型的整体行为。

**如何解读 @fig-shap-summary**:
-   **Y轴 (特征)**: 特征按其**全局重要性**从上到下排序。排在最上面的特征，是模型眼中对判断内容是否"有害"影响最大的特征。
-   **X轴 (SHAP值)**:
    -   `SHAP value > 0`: 表示该特征的存在，会**增加**一篇文章被判定为"有害"的概率。
    -   `SHAP value < 0`: 表示该特征的存在，会**降低**一篇文章被判定为"有害"的概率。
-   **颜色 (特征值)**:
    -   **红色**: 代表该特征本身的值较高（即这个词的TF-IDF分数高，说明它在文档中很重要）。
    -   **蓝色**: 代表该特征本身的值较低（TF-IDF分数低）。
-   **点的分布**: 每个点代表测试集中的一个样本。

**综合解读示例**:
-   假设最顶端的特征是"**自由**"。我们看到，红色的点（即"自由"的TF-IDF值高）几乎全部分布在X轴的正半轴。这说明：**当"自由"这个词出现时（特征值高），它会极大地增加一篇文章被判定为"有害"的概率。** 这完全符合我们对这个虚构模型的预期。
-   假设另一个特征是"**分析**"。我们看到，红色的点（"分析"TF-IDF值高）主要分布在X轴的负半轴。这说明：**当"分析"这个词出现时，它会显著地降低一篇文章被判定为"有害"的概率（即更可能被判定为优质或低质）。**

通过 @fig-shap-summary，我们就能一目了然地掌握模型在识别"有害"类别时，最看重哪些正面和负面词汇，以及这些词汇是如何影响决策的。

## 本节小结

恭喜你！你已经掌握了使用业界主流工具SHAP来剖析黑箱模型的核心技能。

### 🎯 核心技能
1.  **初始化解释器**: 学会了如何为你的模型（特别是树模型）创建一个SHAP解释器。
2.  **局部解释**: 掌握了使用`force_plot`来解释单次预测，并能清晰地解读其含义。
3.  **全局解释**: 掌握了使用`summary_plot`来分析全局特征重要性，并能从多维度解读其丰富信息。

### 🤔 为何重要
这项技能让你真正拥有了与AI"对话"的能力。当模型犯错时，你可以用它来诊断问题；当模型做对时，你可以用它来理解原因；当需要向他人解释时，你可以用它来建立信任。它让你从一个单纯的"调参侠"，成长为一名能够驾驭、理解并改进AI的工程师。

现在，你已经具备了分析和解释模型的全套技能。在本书的最后一节，我们将迎接终极挑战：利用你学到的所有知识，对我们的整个项目进行一次彻底的、生产级别的代码重构。 