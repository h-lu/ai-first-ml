# 10.3 What: 核心概念之词向量与句向量

我们已经直观地感受到了Embedding的魔力。现在，让我们深入一层，为这些概念下一个更精确的定义，并区分两种在实际应用中至关重要的向量：**词向量 (Word Embeddings)** 和 **句向量 (Sentence Embeddings)**。

## 向量是词语的"DNA"

我们可以用一个非常贴切的类比来理解Embedding向量：**向量，就是词语或句子的"DNA"**。

一段DNA（脱氧核糖核酸）是由一系列碱基（A, T, C, G）组成的序列，这个序列中蕴含了一个生物体所有的遗传信息。同样地，一个Embedding向量是由一系列数字组成的序列（例如 `[0.12, -0.45, 0.88, ...]`），这个序列中蕴含了一个词语或句子的**所有语义信息**。

-   **DNA决定了生物特征**: 亲缘关系近的物种（如人和黑猩猩），它们的DNA序列高度相似。
-   **向量决定了语义特征**: 语义关系近的词句（如"国王"和"女王"），它们的向量也高度相似。

正是因为向量成功地将模糊的"语义"编码成了精确的"数学对象"，我们才得以对其进行计算、比较和检索。

## 词向量 (Word Embeddings)

我们在上一节实验中看到的，就是典型的**词向量**。

-   **代表技术**: Word2Vec, GloVe
-   **核心思想**: 为词汇表中的**每一个独立的单词**，都生成一个固定长度的向量。
-   **优点**: 能够很好地捕捉单个词语的语义，以及词与词之间的类比关系（如`king - man + woman ≈ queen`）。
-   **致命弱点**:
    1.  **无法处理未登录词 (Out-of-Vocabulary, OOV)**: 如果一个词在训练模型的词汇表中不存在，它就无法为其生成向量。
    2.  **无法理解上下文**: 它为每个词生成的向量是**静态的、唯一的**。它无法区分下面两句话中"苹果"的不同含义：
        -   "我咬了一口**苹果**。" (水果)
        -   "**苹果**发布了新的手机。" (公司)
        在Word2Vec看来，这两个"苹果"的向量是完全一样的。
    3.  **从词到句的困难**: 如何将一句话中所有词的向量，组合成一个能代表整句话语义的向量？最简单的方法是直接取平均，但这种方法会丢失语序信息，效果往往不佳。

由于这些弱点，单纯的词向量技术已经较少直接用于像RAG这样的下游任务中。

## 句向量 (Sentence Embeddings)

为了解决词向量的局限性，更先进的**句向量**技术应运而生。

-   **代表技术**: Sentence-BERT (SBERT), `all-MiniLM-L6-v2` (我们将在实践中使用的模型)
-   **核心思想**: 不再满足于为单个词编码，而是直接为**一整个句子、段落或文档**生成一个单一的、固定长度的向量。这个向量旨在捕捉整段文本的综合语义。
-   **实现原理**: 这类模型通常基于强大的**Transformer架构**（这也是GPT系列模型的基础）。它们在处理文本时，会充分考虑句子中每个词的上下文，从而能够精准地理解语义。
-   **核心优势**:
    1.  **上下文感知 (Context-Aware)**: 它能够轻易地区分不同语境下的"苹果"，并为它们生成截然不同的向量。
    2.  **处理任意文本**: 它不依赖于固定的词汇表，可以为任何句子（哪怕包含生僻词）生成高质量的向量。
    3.  **直接代表句子语义**: 生成的向量直接代表了整句话的含义，非常适合用于计算句子或段落之间的相似度。

### RAG系统中的选择

在我们的RAG项目中，我们的目标是判断"用户的问题"和"知识库中的文本块"在语义上是否相似。这两者都是句子或段落级别的文本。

因此，**句向量**是我们在这个场景下**不二的选择**。它能够将用户的查询和文档中的每一个文本块，都精准地映射到同一个语义空间中，为我们后续进行高效、准确的相似度检索奠定坚实的基础。

## 本节小结

### 🎯 核心收获
1.  **一个核心类比**: 你掌握了用"词语的DNA"来类比Embedding向量，这有助于你向他人解释其本质。
2.  **清晰区分两种技术**: 你明确了**词向量**和**句向量**的核心区别、各自的优缺点以及代表性技术。
3.  **理解技术选型**: 你知道了在RAG这类"文本匹配"和"语义检索"的任务中，为什么我们必须选择**句向量**而非传统的词向量。

### 🤔 为何重要
理解不同技术之间的演进关系和适用场景，是专业性的体现。你知道了Word2Vec的局限，也就明白了为什么需要BERT这样的Transformer模型；你知道了句向量的优势，也就明白了为什么它是所有现代语义检索系统的基石。这种对技术脉络的把握，将让你在面对未来的新技术时，能够更快地理解其创新点和价值所在。

现在，理论知识已经准备就绪。在下一节，我们将进入激动人心的实践环节，亲手调用一个强大的句向量模型，将我们的第一段文本转化为它在语义空间中的"DNA"——向量。 