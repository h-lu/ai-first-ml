# 10.2 How: 与AI的探索之旅——在几何空间中描绘词语的"灵魂"

::: {.callout-note title="与AI的探索之旅"}

**👤 你:**
你好！我想更直观地理解Embedding。在上一节中，我们认识到需要一种能理解词语"亲戚关系"的技术。Embedding听起来很神奇，我们能亲眼验证一下吗？

**🤖 AI助手:**
绝佳的想法！眼见为实是最好的学习方式。我们可以做一个有趣的实验。我这里有一个预训练好的、包含了大量常见词汇的词向量模型。我们可以从中挑选几个有代表性的词，获取它们的向量，然后使用降维可视化技术（比如t-SNE），将它们绘制在一张二维的图上。你想探索哪些词之间的关系呢？

**👤 你:**
太棒了！我想看一组经典的例子。请帮我获取以下这组词的向量，并可视化它们的位置：
-   `king` (国王)
-   `queen` (女王)
-   `man` (男人)
-   `woman` (女人)

另外，为了做对比，我们再加几个不同领域的词：
-   `paris` (巴黎)
-   `france` (法国)
-   `rome` (罗马)
-   `italy` (意大利)

我很好奇它们在图上会如何分布。

**🤖 AI助手:**
没问题。这是一个非常经典的实验。我已经获取了这些词的向量，并使用t-SNE将它们降维到了二维空间。请看下图，你可以点击代码块上方的摘要来查看用于生成它的Python代码。

#| eval: true
#| echo: false
#| code-fold: true
#| code-summary: "点击查看生成此图表的代码"
#| fig-cap: "词向量在二维空间中的几何关系。我们可以清晰地看到，语义相近的词在空间中聚集，并且不同语义关系（如“性别-皇室”和“国家-首都”）表现为平行的向量。"
#| label: fig-embedding-geometry
import matplotlib.pyplot as plt
import numpy as np

# 为了教学目的，我们在此处模拟t-SNE降维后的二维坐标点
# 核心是让 vector(king) - vector(man) ≈ vector(queen) - vector(woman)
# 以及 vector(paris) - vector(france) ≈ vector(rome) - vector(italy)

# 分组1: 性别-皇室类比
man = np.array([1, 2])
woman = np.array([4, 2])
king = np.array([1, 5])
queen = np.array([4, 5])

# 分组2: 国家-首都类比
france = np.array([-5, -4])
paris = np.array([-2, -3])
italy = np.array([-6, -6])
rome = np.array([-3, -5])

words = ['man', 'woman', 'king', 'queen', 'france', 'paris', 'italy', 'rome']
vectors = {
    'man': man, 'woman': woman, 'king': king, 'queen': queen,
    'france': france, 'paris': paris, 'italy': italy, 'rome': rome
}

# 绘制图形
fig, ax = plt.subplots(figsize=(12, 10))

# 绘制所有点
points = np.array(list(vectors.values()))
ax.scatter(points[:, 0], points[:, 1], s=50, alpha=0.1, color='gray')

# 绘制并标注性别-皇室组
group1_points = np.array([man, woman, king, queen])
ax.scatter(group1_points[:, 0], group1_points[:, 1], s=100, color='royalblue', label='Gender/Royalty Group')
for word in ['man', 'woman', 'king', 'queen']:
    ax.text(vectors[word][0] + 0.2, vectors[word][1], word, fontsize=14, color='darkblue')

# 绘制类比向量
ax.arrow(man[0], man[1], king[0] - man[0], king[1] - man[1],
          head_width=0.2, head_length=0.3, fc='royalblue', ec='royalblue', linestyle='--')
ax.arrow(woman[0], woman[1], queen[0] - woman[0], queen[1] - woman[1],
          head_width=0.2, head_length=0.3, fc='royalblue', ec='royalblue', linestyle='--')

# 绘制并标注国家-首都组
group2_points = np.array([france, paris, italy, rome])
ax.scatter(group2_points[:, 0], group2_points[:, 1], s=100, color='seagreen', label='Country/Capital Group')
for word in ['france', 'paris', 'italy', 'rome']:
    ax.text(vectors[word][0] + 0.2, vectors[word][1], word, fontsize=14, color='darkgreen')

# 绘制类比向量
ax.arrow(france[0], france[1], paris[0] - france[0], paris[1] - france[1],
          head_width=0.2, head_length=0.3, fc='seagreen', ec='seagreen', linestyle='--')
ax.arrow(italy[0], italy[1], rome[0] - italy[0], rome[1] - italy[1],
          head_width=0.2, head_length=0.3, fc='seagreen', ec='seagreen', linestyle='--')

ax.set_title('Word Embeddings Analogies in 2D Space (Simulated)', fontsize=18)
ax.set_xlabel('Semantic Dimension 1', fontsize=14)
ax.set_ylabel('Semantic Dimension 2', fontsize=14)
ax.grid(True, linestyle='--', alpha=0.6)
ax.axvline(0, color='black', linewidth=0.5)
ax.axhline(0, color='black', linewidth=0.5)
ax.legend(fontsize=12)
plt.tight_layout()
plt.show()

**👤 你:**
这太神奇了！我从 @fig-embedding-geometry 中看到了几个非常有趣的现象：
1.  **聚类现象**：代表"人"的四个词聚集在了一个区域，而代表"地理位置"的四个词聚集在了另一个区域。
2.  **国家与首都**：`paris` 和 `france` 靠得很近，`rome` 和 `italy` 也靠得很近，这完全符合常识。
3.  **性别关系**：`king` 和 `queen` 靠得很近，`man` 和 `woman` 也靠得很近。

**🤖 AI助手:**
你观察得非常仔细！但这还不是最神奇的地方。请注意向量之间的**方向和关系**。你试着从 `man` 这个点画一个箭头指向 `king`，再从 `woman` 这个点画一个箭头指向 `queen`。你发现了什么？

**👤 你:**
我明白了！这两条连线（向量）的方向和长度，几乎是**平行且相等**的！这是否意味着，在向量空间中，`从"男人"到"国王"的变化`，和`从"女人"到"女王"的变化`，是可以用同一个向量来表示的？

**🤖 AI助手:**
完全正确！你已经洞察到了词向量最深刻的秘密。这种平行的几何关系，在数学上就表现为美妙的**向量运算**。这正是著名的 `vector('king') - vector('man') + vector('woman') ≈ vector('queen')` 这个公式的几何解释。这个"差值"向量，编码了一种抽象的语义关系——**"皇室"或"权力"**。

同样地，你也可以看到，`从"法国"到"巴黎"的向量`，和 `从"意大利"到"罗马"的向量` 也是几乎平行的。这个向量，编码了**"国家-首都"**这一语义关系。

这种在几何空间中捕捉词语语义关系的能力，正是嵌入（Embedding）技术的魔力所在。它让机器从简单的"识字"迈向了"理解"，为更高级的文本处理任务奠定了基础。

我们刚刚的探索也揭示了：
1.  **距离代表相似度**: 在这个空间里，两个向量的**距离**（或夹角）越近，代表它们在语义上越相似。这就是我们RAG系统中"相似度检索"的根本原理。
2.  **向量运算揭示深层关系**: 向量之间的加减法，可以揭示出词语之间更深层次的、类比性的关系（Analogy）。这使得机器能够进行一定程度的"推理"。

**👤 你:**
我明白了。将抽象的数学概念（高维向量）与直观的几何图形联系起来，是最高效的学习方式。这个几何视角将成为我未来理解所有基于Embedding的技术的"心锚"。

**🤖 AI助手:**
说得太好了。当你再听到"语义相似度"时，你的脑海中浮现的将不再是空洞的文字，而是一幅清晰的、点与点之间远近亲疏的几何图像。

现在我们已经"看到"了Embedding的魔力。在下一节，我们将更深入地探讨它的核心概念，并区分将在我们项目中大显身手的"句向量"和我们刚刚实验的"词向量"有何不同。

:::
