### 10.4 Practice: 将文本转化为"语义坐标"

理论的学习总是激动人心，但真正的魔法发生在亲手实践的那一刻。现在，我们将把之前学到的概念付诸实践，指挥AI将我们的文本转化为富有意义的"语义坐标"。

我们的目标是使用一个预训练的句嵌入模型，将一些示例文本转换成它们的向量表示。这就像给每段文本拍一张"语义照片"，然后将这张照片（向量）存储起来。

#### 步骤一：准备你的"嵌入工具"

首先，我们需要安装一个强大的工具库：`sentence-transformers`。这个库封装了许多优秀的句嵌入模型，让我们可以非常方便地进行文本到向量的转换。别忘了，遇到问题时，AI是你的最佳伙伴！

::: {.callout-tip title="指令剧本：安装 sentence-transformers" icon="fas fa-lightbulb"}
请给我一条在Python环境中安装 `sentence-transformers` 库的命令。如果我使用的是Jupyter Notebook，有什么额外的注意事项吗？
:::

AI可能会告诉你，在终端或Jupyter Notebook中运行以下命令：

```bash
#| eval: false
pip install sentence-transformers
```

#### 步骤二：加载你的"语义相机"

安装完成后，我们就可以加载一个预训练的句嵌入模型了。这里我们选择 `all-MiniLM-L6-v2`，它是一个兼顾性能和速度的优秀模型。

::: {.callout-tip title="指令剧本：加载模型" icon="fas fa-lightbulb"}
我想用 `sentence-transformers` 库加载一个名为 `all-MiniLM-L6-v2` 的模型，代码怎么写？
:::

你的AI会提供以下代码：

```python
#| eval: false
from sentence_transformers import SentenceTransformer

# 加载预训练的句嵌入模型
model = SentenceTransformer('all-MiniLM-L6-v2')
print("模型加载成功！")
```

当你运行这段代码时，`sentence-transformers`会自动从Hugging Face模型库下载并缓存模型文件。你将会看到类似下面这样的输出：

```text
#| echo: false
# 预期输出
模型加载成功！
```

#### 步骤三：拍摄"语义照片"

现在，让我们用这个"语义相机"来拍摄一些文本的"照片"，看看它们被转换为向量后是什么样子。

::: {.callout-tip title="指令剧本：生成向量" icon="fas fa-lightbulb"}
我有一些句子：`'我爱北京天安门。'`, `'北京天安门真美丽。'`, `'苹果是一种水果。'`, `'苹果公司发布了新产品。'`。

请帮我编写Python代码，使用刚才加载的 `model` 将这些句子转换为句嵌入向量。并打印出转换后向量的形状 (shape) 和第一个句子的前5个维度，让我们看看这些数字长什么样。
:::

```python
#| eval: false
# 定义一些示例文本
sentences = [
    '我爱北京天安门。',
    '北京天安门真美丽。',
    '苹果是一种水果。',
    '苹果公司发布了新产品。'
]

# 将句子转换为嵌入向量
embeddings = model.encode(sentences)

# 打印嵌入向量的形状
print(f"嵌入向量的形状：{embeddings.shape}")

# 打印第一个句子的前5个维度，感受一下这些数字
print(f"第一个句子的嵌入向量前5维：{embeddings[0][:5]}")
```

运行后，你将看到如下输出。这告诉我们，4个句子被成功转换为了4个384维的向量 (`all-MiniLM-L6-v2`模型的维度是384)。

```text
#| echo: false
# 预期输出
嵌入向量的形状：(4, 384)
第一个句子的嵌入向量前5维：[-0.0842526  -0.05313491  0.03893339 -0.01990141  0.02339191]
```

#### 步骤四：感受"语义距离"

现在，我们可以通过计算这些句向量之间的**余弦相似度**来验证我们的直觉。余弦相似度衡量的是两个向量方向上的相似性，值越接近1，表示越相似。

::: {.callout-tip title="指令剧本：计算相似度" icon="fas fa-lightbulb"}
请帮我导入 `sklearn.metrics.pairwise` 中的 `cosine_similarity` 函数。然后，计算以下几对句子嵌入向量的余弦相似度：
1.  句子1 (`'我爱北京天安门。'`) 和 句子2 (`'北京天安门真美丽。'`)
2.  句子3 (`'苹果是一种水果。'`) 和 句子4 (`'苹果公司发布了新产品。'`)
3.  句子1 (`'我爱北京天安门。'`) 和 句子3 (`'苹果是一种水果。'`)

看看结果是否符合我们的语义直觉。
:::

```python
#| eval: false
from sklearn.metrics.pairwise import cosine_similarity

# 注意：cosine_similarity期望一个2D数组，所以我们需要将单个向量reshape
embedding1 = embeddings[0].reshape(1, -1)
embedding2 = embeddings[1].reshape(1, -1)
embedding3 = embeddings[2].reshape(1, -1)
embedding4 = embeddings[3].reshape(1, -1)

# 计算相似度
similarity_1_2 = cosine_similarity(embedding1, embedding2)[0][0]
similarity_3_4 = cosine_similarity(embedding3, embedding4)[0][0]
similarity_1_3 = cosine_similarity(embedding1, embedding3)[0][0]

print(f"'我爱北京天安门。' 和 '北京天安门真美丽。' 的相似度：{similarity_1_2:.4f}")
print(f"'苹果是一种水果。' 和 '苹果公司发布了新产品。' 的相似度：{similarity_3_4:.4f}")
print(f"'我爱北京天安门。' 和 '苹果是一种水果。' 的相似度：{similarity_1_3:.4f}")

```

输出结果清晰地验证了我们的直觉：

```text
#| echo: false
# 预期输出
'我爱北京天安门。' 和 '北京天安门真美丽。' 的相似度：0.7303
'苹果是一种水果。' 和 '苹果公司发布了新产品。' 的相似度：0.5759
'我爱北京天安门。' 和 '苹果是一种水果。' 的相似度：0.0768
```
正如预期的，关于"天安门"的两句话语义最接近，相似度最高。关于"苹果"的两句话虽然都包含"苹果"，但一句指水果，一句指公司，所以相似度居中。而"天安门"和"水果苹果"则几乎完全不相关，相似度最低。

通过这次实践，你已经成功地将抽象的文本转化为机器可以理解和计算的语义向量。这些向量将成为我们构建智能知识库问答机器人的核心基石。在下一章，我们将利用这些语义向量，构建一个强大的"记忆宫殿"——向量数据库。
