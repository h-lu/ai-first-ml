# 10.4 Practice: æŒ‡æŒ¥AIè°ƒç”¨æ¨¡å‹å°†æ–‡æ¡£è½¬åŒ–ä¸ºå‘é‡

ç†è®ºå­¦ä¹ å·²ç»ç»“æŸï¼Œç°åœ¨æ˜¯æ—¶å€™äº²è‡ªåŠ¨æ‰‹ï¼Œå°†æ–‡æœ¬çœŸæ­£åœ°è½¬åŒ–ä¸ºå‘é‡äº†ã€‚

åœ¨è¿™ä¸ªå®è·µç¯èŠ‚ï¼Œæˆ‘ä»¬å°†å®ŒæˆRAGè“å›¾"ç¦»çº¿å¤„ç†æµç¨‹"ä¸­çš„å…³é”®ä¸¤æ­¥ï¼š**æ–‡æœ¬åˆ†å— (Splitting)** å’Œ **æ–‡æœ¬å‘é‡åŒ– (Embedding)**ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªéå¸¸å¼ºå¤§ä¸”æµè¡Œçš„å¥å‘é‡åº“`sentence-transformers`ã€‚å®ƒå°è£…äº†å¤§é‡é¢„è®­ç»ƒå¥½çš„ã€é¡¶å°–çš„å¥å‘é‡æ¨¡å‹ï¼Œè®©æˆ‘ä»¬åªéœ€å‡ è¡Œä»£ç ï¼Œå°±èƒ½è°ƒç”¨è¿™äº›å¤æ‚çš„æ¨¡å‹ã€‚

## AIååŒå®è·µï¼šä¸€ä¸ªæŒ‡ä»¤å‰§æœ¬

è¯·æ‰“å¼€ä½ çš„AIç¼–ç¨‹åŠ©æ‰‹ï¼ˆæˆ–ä¸€ä¸ªJupyter Notebookï¼‰ï¼Œè·Ÿéšä¸‹é¢çš„"æŒ‡ä»¤å‰§æœ¬"ï¼Œå‘ä½ çš„AIä¼™ä¼´å‘å‡ºæŒ‡ä»¤ï¼Œä¸€æ­¥æ­¥å®Œæˆæ•´ä¸ªæµç¨‹ã€‚

---

### ç¬¬ä¸€å¹•ï¼šå®‰è£…ä¸å‡†å¤‡

:::{.callout-caution title="ç¬¬ä¸€æ­¥ï¼šè¯·æ±‚AIç»™å‡ºå®‰è£…æŒ‡ä»¤" icon="fas fa-download"}
**ğŸ‘¤ ä½ çš„æŒ‡ä»¤:**

> "ä½ å¥½ï¼Œæˆ‘å‡†å¤‡å¼€å§‹ä¸€ä¸ªä½¿ç”¨å¥å‘é‡çš„é¡¹ç›®ã€‚è¯·ç»™æˆ‘ä¸€æ¡ä½¿ç”¨pipå®‰è£…`sentence-transformers`åº“çš„å‘½ä»¤ã€‚"
:::

**ğŸ¤– AIçš„é¢„æœŸå›ç­”:**
å½“ç„¶ï¼Œè¯·åœ¨ä½ çš„ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```bash
pip install sentence-transformers
```
åŒæ—¶ï¼Œä¸ºäº†æ–¹ä¾¿åç»­çš„æ–‡æœ¬å¤„ç†ï¼Œæˆ‘ä»¬æœ€å¥½ä¹Ÿå®‰è£…`scikit-learn`ï¼Œå®ƒé‡Œé¢åŒ…å«äº†ä¸€äº›æ–¹ä¾¿çš„æ–‡æœ¬åˆ†å‰²å·¥å…·ã€‚
```bash
pip install -U scikit-learn
```

---

### ç¬¬äºŒå¹•ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹

:::{.callout-note title="ç¬¬äºŒæ­¥ï¼šè¯·æ±‚AIç¼–å†™æ¨¡å‹åŠ è½½ä»£ç " icon="fas fa-brain"}
**ğŸ‘¤ ä½ çš„æŒ‡ä»¤:**

> "å¤ªæ£’äº†ï¼Œåº“å·²å®‰è£…ã€‚ç°åœ¨è¯·å¸®æˆ‘ç¼–å†™ä¸€æ®µPythonä»£ç ã€‚æˆ‘éœ€è¦ï¼š
> 1.  ä»`sentence_transformers`åº“ä¸­å¯¼å…¥`SentenceTransformer`ç±»ã€‚
> 2.  åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„å¥å‘é‡æ¨¡å‹ã€‚æˆ‘å¬è¯´`all-MiniLM-L6-v2`æ˜¯ä¸€ä¸ªå¾ˆä¸é”™çš„å…¥é—¨æ¨¡å‹ï¼Œå®ƒåœ¨æ€§èƒ½å’Œé€Ÿåº¦ä¸Šå–å¾—äº†å¾ˆå¥½çš„å¹³è¡¡ã€‚è¯·å¸®æˆ‘åˆå§‹åŒ–è¿™ä¸ªæ¨¡å‹ï¼Œå¹¶èµ‹å€¼ç»™ä¸€ä¸ªåä¸º`embedding_model`çš„å˜é‡ã€‚"
:::

**ğŸ¤– AIçš„é¢„æœŸå›ç­”:**
å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„æ¨¡å‹é€‰æ‹©ã€‚ä¸‹é¢æ˜¯åŠ è½½æ¨¡å‹çš„ä»£ç ï¼š

```python
# å¯¼å…¥SentenceTransformerç±»
from sentence_transformers import SentenceTransformer

# æŒ‡å®šè¦åŠ è½½çš„æ¨¡å‹åç§°
# all-MiniLM-L6-v2 æ˜¯ä¸€ä¸ªåœ¨å¤§é‡æ–‡æœ¬ä¸Šè®­ç»ƒè¿‡çš„ã€é€šç”¨çš„å¥å‘é‡æ¨¡å‹ã€‚
# é¦–æ¬¡è¿è¡Œæ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨ä»Hugging Faceæ¨¡å‹ä¸­å¿ƒä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰ã€‚
model_name = 'all-MiniLM-L6-v2'

# åŠ è½½æ¨¡å‹
embedding_model = SentenceTransformer(model_name)

print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
print(embedding_model)
```

---

### ç¬¬ä¸‰å¹•ï¼šæ–‡æœ¬åˆ†å—ä¸å‘é‡åŒ–

è¿™æ˜¯æˆ‘ä»¬çš„æ ¸å¿ƒä»»åŠ¡ã€‚æˆ‘ä»¬éœ€è¦å°†ä¸€ç¯‡é•¿æ–‡æ¡£ï¼Œå…ˆåˆ‡åˆ†æˆå°å—ï¼Œç„¶åå°†æ¯ä¸ªå°å—éƒ½è½¬æ¢ä¸ºå‘é‡ã€‚

:::{.callout-tip title="ç¬¬ä¸‰æ­¥ï¼šè¯·æ±‚AIç¼–å†™æ–‡æœ¬å¤„ç†ä¸å‘é‡åŒ–å‡½æ•°" icon="fas fa-cogs"}
**ğŸ‘¤ ä½ çš„æŒ‡ä»¤:**

> "éå¸¸æ£’ï¼ç°åœ¨æ˜¯æ ¸å¿ƒç¯èŠ‚ã€‚æˆ‘æœ‰ä¸€æ®µæ¨¡æ‹Ÿçš„é•¿æ–‡æ¡£æ–‡æœ¬ã€‚è¯·å¸®æˆ‘ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°ï¼Œåä¸º`embed_document`ã€‚
>
> **è¿™ä¸ªå‡½æ•°éœ€è¦æ¥æ”¶ä¸¤ä¸ªå‚æ•°ï¼š**
> 1. `text`ï¼šä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»£è¡¨è¦å¤„ç†çš„é•¿æ–‡æ¡£ã€‚
> 2. `model`ï¼šæˆ‘ä»¬åˆšåˆšåŠ è½½çš„`embedding_model`ã€‚
>
> **å‡½æ•°å†…éƒ¨éœ€è¦å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š**
> 1.  ä½¿ç”¨`sklearn.text.RecursiveCharacterTextSplitter`æ¥å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå°å—ï¼ˆChunkï¼‰ã€‚è¯·è®¾ç½®`chunk_size`ä¸º200ï¼ˆæ¯ä¸ªå—æœ€å¤š200ä¸ªå­—ç¬¦ï¼‰ï¼Œ`chunk_overlap`ä¸º20ï¼ˆå—ä¸å—ä¹‹é—´æœ‰20ä¸ªå­—ç¬¦çš„é‡å ï¼Œä»¥ä¿è¯è¯­ä¹‰è¿ç»­æ€§ï¼‰ã€‚
> 2.  è°ƒç”¨`model.encode()`æ–¹æ³•ï¼Œå°†åˆ†å‰²å¥½çš„æ‰€æœ‰æ–‡æœ¬å—ä¸€æ¬¡æ€§åœ°å…¨éƒ¨è½¬æ¢ä¸ºå‘é‡ã€‚
> 3.  **æœ€ç»ˆè¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ–‡æœ¬å—å‘é‡çš„åˆ—è¡¨ã€‚**
>
> è¯·ä¸ºè¿™ä¸ªå‡½æ•°æ·»åŠ æ¸…æ™°çš„æ³¨é‡Šï¼Œå¹¶ç»™æˆ‘ä¸€ä¸ªå®Œæ•´çš„ã€å¯è¿è¡Œçš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬å¦‚ä½•å®šä¹‰ç¤ºä¾‹æ–‡æœ¬å’Œå¦‚ä½•è°ƒç”¨è¿™ä¸ªå‡½æ•°ã€‚"
:::

**ğŸ¤– AIçš„é¢„æœŸå›ç­”:**
è¿™æ˜¯ä¸€ä¸ªéå¸¸å…¸å‹çš„RAGé¢„å¤„ç†æµç¨‹ã€‚ä¸‹é¢æ˜¯å®Œæ•´çš„ä»£ç å®ç°ï¼š

```python
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.text import RecursiveCharacterTextSplitter

# --- 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ ---
# æˆ‘ä»¬å¤ç”¨ä¹‹å‰åŠ è½½å¥½çš„æ¨¡å‹
model_name = 'all-MiniLM-L6-v2'
embedding_model = SentenceTransformer(model_name)
print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")

# --- 2. å‡†å¤‡ç¤ºä¾‹æ–‡æœ¬ ---
# è¿™æ˜¯ä¸€æ®µæ¨¡æ‹Ÿçš„ã€å…³äºRAGçš„ä»‹ç»æ€§é•¿æ–‡æœ¬
long_text = """
Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of large language models (LLMs) with external knowledge retrieval.
The core idea is to retrieve relevant information from a knowledge base before generating a response.
This approach helps to reduce hallucinations, improve factual accuracy, and allow the model to use up-to-date information.
The process typically involves a retriever component that searches for relevant documents or passages, and a generator component (the LLM) that uses this information to craft a final answer.
Vector databases are often used to store and efficiently search through the embedded knowledge.
By grounding the LLM's responses in external data, RAG makes AI systems more reliable and trustworthy.
"""

# --- 3. å®šä¹‰æ–‡æœ¬å¤„ç†ä¸å‘é‡åŒ–å‡½æ•° ---
def embed_document(text, model):
    """
    å°†é•¿æ–‡æ¡£åˆ†å‰²æˆå—ï¼Œå¹¶ä½¿ç”¨æŒ‡å®šæ¨¡å‹å°†æ¯ä¸ªå—è½¬æ¢ä¸ºå‘é‡ã€‚

    å‚æ•°:
    text (str): è¦å¤„ç†çš„é•¿æ–‡æ¡£æ–‡æœ¬ã€‚
    model (SentenceTransformer): ç”¨äºç”Ÿæˆå‘é‡çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚

    è¿”å›:
    list: ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ–‡æœ¬å—å¯¹åº”å‘é‡çš„åˆ—è¡¨ã€‚
    """
    # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
    # chunk_size å®šä¹‰äº†æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°ã€‚
    # chunk_overlap å®šä¹‰äº†å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ã€‚
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
    
    # åˆ†å‰²æ–‡æœ¬
    chunks = text_splitter.split_text(text)
    
    print(f"åŸå§‹æ–‡æœ¬è¢«åˆ†å‰²æˆäº† {len(chunks)} ä¸ªå—ï¼š")
    for i, chunk in enumerate(chunks):
        print(f"å— {i+1}: {chunk}")
    
    # ä½¿ç”¨æ¨¡å‹å¯¹æ‰€æœ‰æ–‡æœ¬å—è¿›è¡Œå‘é‡åŒ–
    # model.encode() æ˜¯ä¸€ä¸ªéå¸¸é«˜æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥ä¸€æ¬¡æ€§å¤„ç†ä¸€ä¸ªæ–‡æœ¬åˆ—è¡¨ã€‚
    embeddings = model.encode(chunks)
    
    return embeddings

# --- 4. è°ƒç”¨å‡½æ•°å¹¶æ£€æŸ¥ç»“æœ ---
document_embeddings = embed_document(long_text, embedding_model)

print(f"\næˆåŠŸç”Ÿæˆäº† {len(document_embeddings)} ä¸ªå‘é‡ã€‚")

# æˆ‘ä»¬å¯ä»¥æ£€æŸ¥ç¬¬ä¸€ä¸ªå‘é‡çš„å½¢çŠ¶å’Œå†…å®¹
first_embedding = document_embeddings[0]
print(f"ç¬¬ä¸€ä¸ªå‘é‡çš„ç»´åº¦æ˜¯: {first_embedding.shape}")
print(f"ç¬¬ä¸€ä¸ªå‘é‡çš„å‰5ä¸ªå€¼æ˜¯: {first_embedding[:5]}")

# å°†æ‰€æœ‰å‘é‡ç»„åˆæˆä¸€ä¸ªNumPyçŸ©é˜µ
embedding_matrix = np.array(document_embeddings)
print(f"\næ‰€æœ‰å‘é‡ç»„æˆçš„çŸ©é˜µå½¢çŠ¶ä¸º: {embedding_matrix.shape}")
```

## æœ¬èŠ‚å°ç»“

æ­å–œä½ ï¼ä½ å·²ç»æˆåŠŸåœ°å®Œæˆäº†æˆ‘ä»¬RAGç³»ç»Ÿä¸­æœ€æ ¸å¿ƒçš„æŠ€æœ¯ç¯èŠ‚ä¹‹ä¸€ã€‚

### ğŸ¯ æ ¸å¿ƒæ”¶è·
1.  **ä¸€ä¸ªå®ç”¨çš„ä»£ç ç‰‡æ®µ**: ä½ è·å¾—äº†ä¸€ä¸ªå¯ä»¥ç›´æ¥å¤ç”¨çš„Pythonå‡½æ•°ï¼Œå®ƒæ•´åˆäº†**æ–‡æœ¬åˆ†å—**å’Œ**å¥å‘é‡ç”Ÿæˆ**è¿™ä¸¤ä¸ªå…³é”®æ­¥éª¤ã€‚
2.  **æŒæ¡ä¸€ä¸ªæ ¸å¿ƒåº“**: ä½ å­¦ä¼šäº†å¦‚ä½•ä½¿ç”¨`sentence-transformers`åº“æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶è°ƒç”¨`.encode()`æ–¹æ³•æ¥æ‰§è¡Œå‘é‡åŒ–ã€‚
3.  **å®ŒæˆRAGè“å›¾çš„å…³é”®ä¸€æ­¥**: ä½ äº²æ‰‹å®ç°äº†RAG"ç¦»çº¿å¤„ç†æµç¨‹"ä¸­çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œä¸ºæˆ‘ä»¬ä¸‹ä¸€ç« æ„å»ºå‘é‡æ•°æ®åº“åšå¥½äº†æ•°æ®å‡†å¤‡ã€‚

### ğŸ¤” ä¸ºä½•é‡è¦
"çº¸ä¸Šå¾—æ¥ç»ˆè§‰æµ…ï¼Œç»çŸ¥æ­¤äº‹è¦èº¬è¡Œ"ã€‚é€šè¿‡äº²æ‰‹ç¼–å†™å¹¶è¿è¡Œä»£ç ï¼Œä½ å°†ç†è®ºçŸ¥è¯†è½¬åŒ–ä¸ºäº†å®é™…çš„æŠ€èƒ½ã€‚ä½ ä¸å†åªæ˜¯"çŸ¥é“"Embeddingæ˜¯ä»€ä¹ˆï¼Œè€Œæ˜¯"ä¼šç”¨"Embeddingæ¥å¤„ç†ä½ è‡ªå·±çš„æ–‡æœ¬æ•°æ®ã€‚

æˆ‘ä»¬ç°åœ¨æ‹¥æœ‰äº†ä¸€æ‰¹é«˜è´¨é‡çš„ã€è•´å«è¯­ä¹‰çš„æ–‡æ¡£å‘é‡ã€‚ä½†å®ƒä»¬è¿˜åªæ˜¯é™é™åœ°èººåœ¨å†…å­˜é‡Œã€‚åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä¸ºè¿™äº›å‘é‡å»ºç«‹ä¸€ä¸ªé«˜æ•ˆçš„"è®°å¿†å®«æ®¿"â€”â€”å‘é‡æ•°æ®åº“ï¼Œè®©å®ƒä»¬èƒ½å¤Ÿè¢«éšæ—¶ã€å¿«é€Ÿåœ°æ£€ç´¢ã€‚ 