# 14.3 What: 核心概念：监督微调 (SFT)

::: {.callout-tip title="核心概念：监督微调 (Supervised Fine-tuning)"}

**一句话定义：**
监督微调（SFT）是一种让预训练好的大语言模型"学会"特定领域知识、任务格式或沟通风格的技术，其核心是让模型模仿一组高质量的"输入-输出"范例（例如，"问题-标准答案"）。

---

**生动的类比："模仿专家"**

想象一下，一个顶级的模仿演员，他能模仿世界上任何人的声音和举止，这就是我们强大的**基础大语言模型（Base LLM）**。现在，我们希望他能扮演"咖啡豆奇旅的金牌客服"这个特定角色。

我们该怎么做呢？

我们会给他一本"**剧本**"，这本剧本就是我们的**SFT数据集**。剧本里写满了各种场景下的对话：

*   **场景（输入/Prompt）:** "顾客问：我不太懂咖啡，有什么推荐吗？"
*   **台词（输出/Completion）:** "金牌客服回答：完全没问题！如果您喜欢柔和一些的口感……"

演员（模型）的任务，就是反复地、逐字逐句地去模仿剧本里的台词。在SFT的训练过程中，模型会生成自己的"台词"，然后和剧本上的"标准台词"进行对比。

*   如果模型说出的台词和剧本**一模一样**，很好，保持住。
*   如果模型说出的台词和剧本**有出入**，一个名为"**损失函数（Loss Function）**"的内部导演就会给出"负反馈"，并通过**反向传播（Backpropagation）**调整演员的"表演技巧"（模型的内部参数），让他下一次的模仿更接近剧本。

经过成千上万次这样的"排练"，这位演员（模型）最终就能完美地"入戏"，将剧本内化于心。当他再遇到剧本里的场景（问题）时，就能脱口而出我们期望的"台词"（回答）。更重要的是，他还能举一反三，在遇到剧本之外的、但类似的场景时，也能用我们期望的风格和口吻来即兴发挥。

这就是SFT的魔力：**通过高质量的模仿，实现高效的知识和风格迁移。**

---

**核心工具：TRL `SFTTrainer`**

为了实现这个"排练"过程，Hugging Face的`trl`（Transformer Reinforcement Learning）库为我们提供了一个极其强大的工具：`SFTTrainer`。

你可以把它想象成一位专业的"**表演教练**"。你只需要：

1.  把"**演员**"（你的基础LLM）告诉他。
2.  把"**剧本**"（你的SFT数据集）交给他。
3.  设定一些"**排练计划**"（训练参数，如排练多少次、学习速度多快）。

`SFTTrainer`就会自动处理所有底层的复杂工作，例如数据处理、损失计算、模型优化等等，让你能以最高效的方式，完成对AI的"岗前培训"。
::: 