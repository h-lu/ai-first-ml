# 11.3 What: 核心概念之相似度计算与近似最近邻(ANN)

在上一节，我们理解了ANN的核心是一种"权衡"的智慧。现在，我们来深入了解一下实现这种智慧的两个具体的技术基石。

## 基石一：如何计算向量的"相似度"？

我们一直在说"相似度"，这在数学上到底是如何计算的？对于高维向量，最常用的相似度度量标准有两个：

1.  **欧氏距离 (Euclidean Distance)**: 这是我们在二维或三维空间中最直观的距离概念，就是两点之间的直线距离。距离越短，向量越相似。
2.  **余弦相似度 (Cosine Similarity)**: 它衡量的是两个向量在方向上的相似程度，即它们之间夹角的余弦值。夹角越小，余弦值越接近1，代表两个向量指向的方向越一致，语义越相似。

![一个对比欧氏距离和余弦相似度的图。图中有三个点O, A, B。OA和OB的欧氏距离是线段AB的长度。而它们的余弦相似度，则只与角AOB有关，与OA和OB的长度无关。](https://i.imgur.com/8Fk4V9S.png)
*图11.2: 欧氏距离 vs. 余弦相似度*

### 为何在RAG中更常用余弦相似度？

在绝大多数文本语义相关的应用中（包括我们的RAG），**余弦相似度**是更受青睐的选择。为什么？

因为在语义的世界里，我们通常更关心内容的**"主题"或"方向"**，而不是其"强度"或"篇幅"。

例如，有两段话：
-   **A**: "RAG系统通过检索增强生成来提升性能。" (短句)
-   **B**: "检索增强生成（RAG）是一种先进的人工智能技术，它通过在生成答案之前，从外部知识库中检索相关信息，来显著提升大型语言模型的性能、减少幻觉并确保答案的时效性。" (长句)

这两段话的核心语义高度一致。如果用余弦相似度来计算，它们的向量夹角会非常小，相似度会很高。但如果用欧氏距离，B向量因为包含了更多信息，它的"长度"（模）可能会比A向量长很多，导致它们的欧氏距离较大。

因此，**余弦相似度**能够更好地捕捉到与文本长度无关的纯粹的语义相似性，是我们在RAG项目中进行相似度计算的**首选**。

## 基石二：更聪明的ANN算法——HNSW

我们在上一节用"聚类"来类比ANN的原理。在实际的向量数据库中，除了基于聚类的方法（如FAISS中的`IndexIVFFlat`），还有一种更流行、性能也往往更好的算法——**HNSW (Hierarchical Navigable Small World, 分层可导航小世界)**。

如果说基于聚类的算法像是在"给城市划分街区"，那么HNSW则更像是在"**构建一个高效的社交关系网络**"。

### HNSW的直观类比：在社交网络里找人

想象一下，你想在一个全球拥有数十亿用户的社交网络（比如LinkedIn）上，找到一位你只知道其大概背景（比如"一位在北京做AI研究的专家"）的特定人物。

-   **暴力搜索**: 你不可能去查看数十亿用户的个人主页。
-   **HNSW策略**: 你会这样做：
    1.  **从高层网络开始 (高速公路)**: 你可能会先从你自己的好友列表里，找到一位在AI领域的"大V"或"连接点"（比如李开复）。这是一个高层级的、稀疏的"高速公路"网络。
    2.  **进入局部网络 (城市公路)**: 通过这位"大V"，你进入了他的好友圈。这个圈子里的人，大部分都和AI相关。你在这个更小的、更密集的网络里继续寻找，可能会找到一位"在北京AI圈颇有影响力的人"。
    3.  **精细查找 (街区小路)**: 通过这位"影响力人物"，你进一步缩小范围到他的好友列表。在这个更小、更精确的网络里，你最终找到了你要找的那位专家。

**HNSW的核心思想就是构建这样一个多层的、从稀疏到稠密的图网络结构：**

-   **高层图 (Layer 1, 2, ...)**: 节点很少，但连接的都是"长距离"的边，像高速公路一样，让你可以在图上快速"跳跃"，迅速接近目标区域。
-   **底层图 (Layer 0)**: 包含了所有的向量节点，连接非常稠密，像城市的毛细血管网络一样，让你可以在目标区域内进行精细的查找。

![一个HNSW的多层图结构示意图。最上层只有几个节点和长连接，越往下层节点越多，连接越密集。查询从顶层入口点开始，逐层向下，最终在最底层找到最近的邻居。](https://i.imgur.com/kS4fJ5Q.png)
*图11.3: HNSW多层网络结构示意图*

在查询时，算法从顶层最高速的"高速公路"的某个入口点进入，一路"导航"向下，最终在最底层的"街道网络"中找到离查询向量最近的邻居。这种策略被证明在速度和精度之间取得了极佳的平衡，是目前最高效的ANN算法之一。

## 本节小结

### 🎯 核心收获
1.  **掌握核心度量**: 你理解了**余弦相似度**的含义，并知道了为什么它比欧氏距离更适合用于语义检索。
2.  **一个更高级的类比**: 你掌握了用"社交网络找人"来类比**HNSW算法**，理解了它通过构建多层网络来实现高效查找的原理。
3.  **了解技术细节**: 你对向量数据库内部的工作机制有了更具体、更深入的了解，不再只是一个"黑箱"。

### 🤔 为何重要
理解这些核心概念，能让你在使用向量数据库时，做出更专业的决策。例如，当你在配置一个向量数据库索引时，如果看到`metric_type`（度量类型）这个参数，你会毫不犹豫地选择`cosine similarity`；如果看到`index_type`（索引类型）这个参数，并且有`HNSW`这个选项时，你会知道它通常是一个高性能的选择。

理论学习已经足够。在下一节，我们将把所有理论付诸实践，亲手用代码构建一个向量索引，并完成一次激动人心的检索任务。 