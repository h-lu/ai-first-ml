---
title: "15.5 Challenge: 改变环境或奖励，观察策略变化"
---

## 成为"上帝"，改变你的世界

**【AI导演】**

> **场景**: 你的定价机器人取得了巨大的成功，它为你设计的"标准世界"找到了近乎完美的定价策略。但真实世界是多变的。如果出现了一个强大的竞争对手，或者市场需求突然因为某个事件而激增，你的智能体还能适应吗？
>
> 现在，轮到你来扮演"上帝"的角色了。你将亲手修改这个模拟世界的规则，然后观察我们已经训练好的智能体，或者一个重新训练的智能体，其行为会发生怎样有趣的变化。
>
> 这不仅仅是一个练习，它触及了强化学习在现实世界应用中的核心问题：**模型的泛化能力和对环境变化的适应性**。
>
> **你的任务**: 选择以下至少一个挑战，向你的AI助手提出修改代码的请求，重新运行训练，并对比新的策略热力图与旧的有什么不同，然后尝试解释其原因。

---

### 挑战1：引入竞争对手

一个精明的定价策略师，必须时刻关注竞争对手的动向。

**【给AI的提示词】**

> 请帮我修改 `PricingSimulator` 的 `step` 方法。我们来引入一个"竞争对手价格 (`competitor_price`)""因素。
>
> 修改需求函数：
>
> 原来的需求函数是 `demand = self.demand_factor / price`。
>
> 现在，我们假设当我们的价格 `price` 高于 `competitor_price` 时，我们会损失一部分需求。新的需求函数可以设计为：
>
> ```python
> competitor_price = 1100 # 假设竞争对手一直卖1100元
> price_difference = price - competitor_price
> 
> # 如果我们比对手贵，需求会下降
> if price_difference > 0:
>     demand_multiplier = 0.5 
> else:
>     demand_multiplier = 1.0
> 
> # 新的需求计算
> base_demand = self.demand_factor / price
> demand = base_demand * demand_multiplier * np.random.uniform(0.8, 1.2)
> ```
>
> 请将这段逻辑整合进 `step` 方法中。然后，我将使用完全相同的训练代码，重新训练一个智能体。

**【需要你思考和分析的问题】**

1.  **预测**: 在你看到新的热力图之前，请先思考一下：引入这个竞争对手后，智能体的定价策略可能会发生什么变化？它会在哪些状态下更倾向于选择低于1100元的价格？
2.  **观察**: 运行训练，生成新的策略热力图。
3.  **分析**: 对比新旧两张热力图。你的预测准确吗？智能体是否学会了"看人下菜碟"？在哪些情况下，它仍然会选择高于1100元的价格？为什么？（提示：可能是在剩余座位很少，不愁卖的时候）。

---

### challenge 2：改变奖励函数 (引入库存成本)

一个好的企业不仅要考虑收入，还要考虑成本。如果每个未售出的座位都有维护成本呢？

**【给AI的提示词】**

> 请帮我再次修改 `PricingSimulator` 的 `step` 方法。这一次，我们要在**奖励函数**上做文章。
>
> 在 `step` 方法的最后，当模拟结束时（`done` is `True`），我们需要对最终的奖励进行一次性调整。
>
> 修改逻辑如下：
> 1.  在 `step` 方法的返回部分，找到 `if done:` 的逻辑判断。
> 2.  如果 `done` 为 `True`，计算最终的库存成本 `storage_cost = self.remaining_seats * 200` (假设每个剩余座位最终会产生200元的成本)。
> 3.  从当天的奖励 `reward` 中减去这个 `storage_cost`。
>
> ```python
> # ... step方法计算完reward之后
> 
> done = self.remaining_days == 0 or self.remaining_seats == 0
> final_reward = reward
> if done and self.remaining_seats > 0:
>     storage_cost = self.remaining_seats * 200 # 每个空位的成本
>     final_reward -= storage_cost
> 
> # 在返回时，使用 final_reward
> return next_state, final_reward, done, {}
> ```
>
> 请将这段逻辑整合进 `step` 方法中，然后我将重新训练模型。

**【需要你思考和分析的问题】**

1.  **预测**: 引入库存成本后，智能体的行为会如何改变？它会变得更"激进"还是更"保守"？它会更倾向于在早期以较低价格卖出机票，还是坚持高价策略？
2.  **观察**: 运行训练，生成新的策略热力图。
3.  **分析**: 对比新旧两张热力图。智能体的策略是否真的变得更倾向于"清仓"了？在哪些状态下，这种变化最为明显？这个新的奖励函数，是否成功地教会了智能体"风险厌恶"？

---

> **学习者笔记**:
>
> 完成这些挑战，你将深刻地理解到，在强化学习中：
>
> *   **环境决定了问题的本质。**
> *   **奖励函数定义了智能体的"价值观"和最终目标。**
>
> 能够根据实际问题，灵活地调整环境模型和设计精巧的奖励函数，是高级RL工程师与初学者的核心区别。这也是将RL思想应用到真实世界问题的关键所在。 