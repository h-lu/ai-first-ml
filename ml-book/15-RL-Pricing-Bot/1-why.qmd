---
title: "15.1 Why: 为何动态定价是RL的经典入门战场？"
---

## 一个价值百万美元的问题

::: {.callout-note title="场景设定：【AI导演】"}
**场景**: 想象一下，你是一家热门航空公司的收益管理部门总监。你们即将发售一趟从北京到三亚的春节期间航班，共有150个座位。

距离起飞还有30天。你的任务是：**设计一个定价策略，以实现这趟航班的总收入最大化。**

这是一个看似简单，实则极其复杂的问题。

*   **定价太高**: 可能会吓跑大量潜在客户，导致航班起飞时还有大量空座位，损失惨重。
*   **定价太低**: 机票可能在几天内就**售罄**，虽然看起来很成功，但你损失了大量的"本可以多赚"的钱，因为后面还有很多愿意出高价的商务人士或最后一刻才决定出行的旅客。

更复杂的是，这个问题是**动态**的。最优的价格取决于**时间**（距离起飞还有多少天）和**库存**（还剩多少空座位）。

你会如何解决这个问题？
:::

---

### 为什么传统方法不够好？

你可能会想，我们可以用监督学习来解决。比如，收集过去几年所有航班的定价历史和销售数据，然后训练一个模型来预测在"给定时间和库存"下，哪个价格能带来最多的销售。

这个思路有几个致命的缺陷：

1.  **无法探索未知**: 监督学习只能从历史数据中学习。它永远无法告诉你，一个**从未尝试过**的新价格（比如比历史上任何时候都高10%）会带来什么结果。它被过去的经验束缚住了。
2.  **忽略长期回报**: 监督学习模型可能会告诉你，今天降价50元能多卖出10张票，这看起来是个好决策。但它无法告诉你，这个决策会导致机票过早售罄，从而损失了后面愿意出高价的客户，最终导致总收入降低。它追求的是**瞬时最优**，而非**全局最优**。
3.  **环境是动态的**: 你的定价本身就会影响消费者的行为，从而改变未来的环境（剩余库存）。这种"行动"与"环境"之间的相互作用，是监督学习无法建模的。

### 强化学习的完美舞台

现在，让我们用上一章学习的"强化学习四要素"来重新解构这个问题：

1.  **智能体 (Agent)**: 你的**定价机器人**。
2.  **环境 (Environment)**: 整个机票销售市场，包括时间、库存和顾客需求模型。
3.  **状态 (State)**: 智能体做决策时需要的所有信息。为了简化问题，我们可以定义状态为一个元组：`(剩余天数, 剩余座位数)`。例如，`(30, 150)` 就是初始状态。
4.  **动作 (Action)**: 智能体可以采取的定价决策。我们可以定义一个离散的动作空间，比如：`[500元, 800元, 1000元, 1200元, 1500元]`。
5.  **奖励 (Reward)**: 智能体做出一个定价决策后，当天产生的**销售收入**。例如，定价1000元，卖出了5张票，那么当天的奖励就是 `5000`。

**智能体的目标**: 在整个销售周期（30天）结束时，最大化**累积奖励**（即总销售收入）。

---

::: {.callout-tip title="学习者笔记"}
看到了吗？动态定价问题完美地契合了强化学习的框架：
-   它需要在**一系列时间点**上做决策。
-   每个决策都会影响**未来的状态**。
-   目标是最大化一个**长期的、累积的**回报。
-   它需要在**利用（Exploitation）**已知的高效价格和**探索（Exploration）**未知价格之间找到平衡。

这使得它成为了我们学习和实践RL算法的理想"沙盒"。在下一节，我们将亲自扮演"上帝"，指挥AI来创造这个沙盒世界。
::: 