---
title: "15.3 How/What: 与AI一起实现Q-Learning算法"
---

## 智能体的"决策地图": Q-Table

**【AI导演】**

> **场景**: 我们的"训练场"(`PricingSimulator`)已经准备就绪。现在，我们需要创造我们的"运动员"——那个即将学习动态定价的智能体。
>
> 它的核心问题是：**在每个特定状态下(比如还剩15天，100个座位)，我应该选择哪个价格(动作)，才能最终获得最大的总收入？**
>
> 智能体需要一张"地图"来指导它的决策。这张地图，就是我们即将学习的**Q-Table**。你将再次通过与AI的对话，一步步揭开它的神秘面纱，并理解其背后的更新法则——**Q-Learning**。
>
> **AI Copilot，启动！**

---

### **【与AI的对话】**

**你**:

> 环境已经有了，现在我该怎么创建一个智能体，让它在这个环境里学习呢？它需要一个"大脑"来做决策吧？

**AI助手**:

> 问得好！智能体的"大脑"就是它的**策略(Policy)**。对于我们这个相对简单的问题，我们可以用一种非常经典且直观的方式来表示这个策略，那就是 **Q-Table**。
>
> 想象一下，我们有一张巨大的表格。
>
> *   **表格的每一行**，代表一个智能体可能遇到的**状态 (State)**，也就是 `(剩余天数, 剩余座位数)` 的组合。
> *   **表格的每一列**，代表一个智能体可以采取的**动作 (Action)**，也就是我们预设的几种价格 `[500, 800, ...]`。
> *   **表格中每个单元格里的值**，我们称之为 **Q值 (Q-value)**。它代表的是：**在"该行对应的状态下"，采取"该列对应的动作"，预计未来能获得的"总回报"的期望值是多少**。
>
> 这张表，就是智能体的决策地图。

**你**:

> 听起来很有趣。所以，如果智能体处在某个状态，它只需要查看Q-Table中对应的**那一行**，然后选择**Q值最高**的那个动作(价格)去做，就是当前的最优决策了？

**AI助手**:

> 完全正确！这就是Q-Table的用法。它为智能体在任何状态下选择最佳动作提供了直接的依据。
>
> 但这里有一个关键问题：我们一开始并不知道这张表里的值应该是多少。在学习开始前，这张表是空的(或者用0来填充)。
>
> 智能体的学习过程，本质上就是通过在环境中不断地**试错 (Trial and Error)**，来逐步**填充和更新**这张Q-Table的过程。

**你**:

> 那它具体是怎么"更新"这张表的呢？肯定有一个数学公式吧？

**AI助手**:

> 是的，这就是强化学习中最负盛名的公式之一：**Q-Learning 更新公式**。
>
> 当智能体在状态 `S`，采取了动作 `A`，然后得到了奖励 `R`，并进入了新状态 `S'`之后，它就会用下面的公式来更新Q-Table中 `Q(S, A)` 的值：
>
> \[ Q(S, A) \leftarrow (1 - \alpha) \cdot Q(S, A) + \alpha \cdot \left( R + \gamma \cdot \max_{a'} Q(S', a') \right) \]
>
> 这个公式看起来有点吓人，但别怕，我们把它拆解一下，你会发现它非常符合直觉。

---

### What: 核心概念之Q-Learning

让我们来"翻译"一下这个公式。`Q(S, A)` 的新值，是由两部分加权平均而来的：

1.  **`(1 - α) * Q(S, A)`**: 这是**"旧的认识"**。我们不能完全抛弃过去学到的经验，所以我们保留一大部分旧的Q值。`α` (alpha) 是**学习率 (Learning Rate)**，通常是一个0到1之间的小数(比如0.1)。它控制了我们每次更新的步子有多大。

2.  **`α * ( ... )`**: 这是**"新的发现"**。我们用新的探索结果来修正我们旧的认识。这部分又分为两个小部分：
    *   **`R`**: 这是我们采取动作后得到的**即时奖励 (Immediate Reward)**。这是最直接、最真实的反馈。
    *   **`γ * max Q(S', a')`**: 这是对**未来奖励的估算 (Estimated Future Reward)**。
        *   `max Q(S', a')` 的意思是：在新状态 `S'` 下，环顾四周，看看从这个新位置出发，能走到的"最有价值"的下一步在哪里(也就是所有可能的新动作 `a'` 中，Q值最大的那个)。
        *   `γ` (gamma) 是**折扣因子 (Discount Factor)**，也是一个0到1之间的数(比如0.95)。它代表了我们对"未来"的看重程度。`γ` 越接近1，说明我们越有"远见"，越看重未来的长期回报；`γ` 越接近0，说明我们越"短视"，只关心眼前的即时奖励。

**用一句话来概括Q-Learning的更新哲学就是：**

> 一个位置(状态S)的价值，等于我挪了一步(动作A)后**直接拿到的好处(奖励R)**，**加上**，我到达的**新位置(状态S')未来的潜力(max Q)**。

---

> **学习者笔记**:
>
> *   **Q-Table**: 智能体的决策"地图"，存储了在每个状态下执行每个动作的预期长期回报。
> *   **Q-Learning**: 一种更新Q-Table的方法，它巧妙地结合了**即时奖励**和对**未来奖励的估算**，使得智能体能够在没有完整环境模型的情况下，通过试错来学习最优策略。
> *   **超参数 (Hyperparameters)**: **学习率α** 和 **折扣因子γ** 是需要我们手动设置的关键参数，它们会显著影响智能体的学习效率和最终性能。
>
> 在下一节，我们将把这个理论变成代码，编写主训练循环，让我们的定价机器人真正"活"起来。 