---
title: "15.4 Practice (2): 启动训练并让AI可视化Q-Table"
---

## 见证"学习"的发生

**【AI导演】**

> **场景**: 理论的铺垫已经完成，代码的舞台 (`PricingSimulator`) 也已搭建。现在，是时候让我们的主角——Q-Learning智能体——登场，并开始它史诗般的学习过程了。
>
> 你的任务是：**再次指挥你的AI编程助手，编写一个完整的主训练循环。** 在这个循环中，智能体将与环境进行数万次交互，从一个"什么都不懂"的随机探索者，逐渐成长为一个"精明"的定价策略师。最后，我们将打开它的"大脑"，将它学到的Q-Table可视化出来，亲眼见证智能的涌现。
>
> **AI Copilot，启动！**

---

### 指令剧本：编写主训练循环

我们将把整个训练过程封装在一个Python脚本中。请继续在你的Jupyter Notebook或VS Code中操作。

#### **第一步：初始化所有组件**

在开始训练前，我们需要把环境、Q-Table和所有超参数都准备好。

**【给AI的提示词】**

> 请帮我编写Q-Learning训练的初始化部分。需要完成以下工作：
>
> 1.  导入 `numpy` 和 `matplotlib.pyplot` 以及 `seaborn` 库。
> 2.  实例化我们之前创建的 `PricingSimulator` 环境。
> 3.  **初始化Q-Table**: 创建一个巨大的Numpy数组来作为Q-Table。
>     *   它的维度应该是 `(状态数量, 动作数量)`。
>     *   状态数量是 `(总天数 + 1) * (总座位数 + 1)`。
>     *   动作数量是 `len(env.action_space)`。
>     *   用 `np.zeros` 将所有初始Q值设为0。
> 4.  **定义超参数**:
>     *   `learning_rate` (α) = 0.1
>     *   `discount_factor` (γ) = 0.95
>     *   `episodes` (训练的总轮数) = 20000
>     *   `epsilon` (探索率) = 1.0 (初始时完全探索)
>     *   `epsilon_decay` (探索率衰减) = 0.999
>     *   `min_epsilon` (最小探索率) = 0.01

---

#### **第二步：实现"探索"与"利用"策略**

智能体不能总是选择当前看起来最好的动作（利用），有时也需要尝试一些未知的选择（探索），这就是著名的"探索-利用困境"。我们将使用一种简单而经典的 **Epsilon-Greedy** 策略。

**【给AI的提示词】**

> 接下来，请帮我实现Epsilon-Greedy策略。
>
> 编写一个名为 `choose_action` 的函数，它接收 `state` 和当前的 `epsilon` 作为输入。
>
> 1.  生成一个0到1之间的随机数。
> 2.  如果这个随机数**大于** `epsilon`，我们就进行"**利用 (Exploitation)**"：在Q-Table中找到当前状态 `state` 对应的那一行，选择并返回Q值最大的那个动作的索引 `np.argmax(q_table[state_index])`。
> 3.  否则（随机数小于等于 `epsilon`），我们就进行"**探索 (Exploration)**"：随机选择一个动作并返回其索引 `env.action_space.sample()` (哦，我们的环境类里还没有这个，可以直接用 `np.random.randint(len(env.action_space))`)。

---

#### **第三步：编写核心训练循环**

这是所有魔法发生的地方。智能体将在这里与环境交互，并根据Q-Learning公式更新自己的认知。

**【给AI的提示词】**

> 现在，请帮我编写核心的主训练循环。
>
> 1.  用 `for episode in range(episodes):` 开始循环。
> 2.  在每轮循环开始时，用 `env.reset()` 重置环境，获取初始状态 `state`。
> 3.  用一个 `while not done:` 循环来模拟一整次销售过程（直到售罄或时间结束）。
> 4.  在 `while` 循环内部：
>     a.  将 `state`（一个元组）转换为Q-Table中的行索引 `state_index`。计算方法是 `state[0] * (env.total_seats + 1) + state[1]`。
>     b.  调用 `choose_action` 函数来选择一个动作 `action`。
>     c.  让环境执行这个动作：`next_state, reward, done, _ = env.step(action)`。
>     d.  计算新状态的索引 `next_state_index`。
>     e.  获取新状态下的最大Q值 `max_future_q = np.max(q_table[next_state_index])`。
>     f.  **应用Q-Learning公式**: `q_table[state_index, action] = (1 - learning_rate) * q_table[state_index, action] + learning_rate * (reward + discount_factor * max_future_q)`。
>     g.  更新当前状态：`state = next_state`。
> 5.  在每轮 `episode` 结束后，**更新探索率 `epsilon`**: `epsilon = max(min_epsilon, epsilon * epsilon_decay)`。这样智能体就会在学习初期多探索，后期多利用。
> 6.  （可选）可以每隔1000轮打印一次当前的 `episode` 和 `epsilon`，方便我们观察训练进度。

---

#### **第四步：可视化Q-Table**

训练结束后，我们的Q-Table里就充满了智能体学到的"智慧"。但它是一个巨大的数字矩阵，并不直观。我们需要将它可视化。

**【给AI的提示词】**

> 训练循环代码完成后，请帮我编写可视化的部分。
>
> 1.  我们的Q-Table是二维的 `(状态, 动作)`，但状态本身是由 `(天数, 座位数)` 两个维度构成的。为了可视化，我们只看在每个状态下，哪个动作的Q值最高。创建一个新的2D数组 `policy_table`，其维度是 `(总天数 + 1, 总座位数 + 1)`。
> 2.  遍历所有可能的天数和座位数，对于每个状态，找出Q-Table中Q值最大的动作索引，并存入 `policy_table`。
> 3.  最后，使用 `seaborn.heatmap()` 函数将这个 `policy_table` 绘制成一张热力图。
> 4.  给图表加上清晰的标题、X轴标签（剩余座位数）和Y轴标签（剩余天数）。

---

点击查看最终的热力图可能的样子


*(这是一个示例图片，你的实际输出颜色和模式可能会有所不同，但应该能看出清晰的策略分区)*

**图解读**: 这张热力图就是智能体学到的最终策略。图中的不同颜色代表了不同的定价决策（比如深色代表高价，浅色代表低价）。你可以清晰地看到：

*   **当剩余天数很多，座位也很多时（左上角）**，智能体倾向于采取**较低价格**，以求薄利多销，快速回笼资金。
*   **当剩余天数很少，但座位依然很多时（左下角）**，智能体可能会采取**极低价格**，进行最后的清仓甩卖。
*   **当剩余天数和座位数都比较充裕时（中间区域）**，它会采取**中等价格**。
*   **当剩余座位数变得稀少时（右侧区域）**，无论剩余多少天，智能体都果断地采取**最高价格**，因为此时"物以稀为贵"，它要榨取最大的利润。

这就是智能。它不是被硬编码的规则，而是通过上万次模拟试错，自主学习到的最优策略。

---

> **学习者笔记**:
>
> 恭喜你！你已经完整地走完了一个强化学习项目的核心流程。你不仅构建了环境，还成功地训练了一个能自主学习的智能体，并"解剖"了它的大脑，理解了它的决策逻辑。
>
> 在最后一节，我们将进行一个有趣的挑战：改变游戏规则，看看我们的智能体是否足够"聪明"，能够适应新的世界。 