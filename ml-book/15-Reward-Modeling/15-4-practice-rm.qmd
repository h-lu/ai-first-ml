# 15.4 Practice: 训练"咖啡品味裁判"

理论和方法都已清晰，现在是我们亲手"培养"出"咖啡豆奇旅"项目专属的"品味裁判"的时刻了。

我们将再次通过一个完整的"指令剧本"，指挥AI使用`trl`库中的`RewardTrainer`，完成从数据准备、模型加载到训练和验证的全过程。

---

## AI协同实践：一个完整的RM训练指令剧本

### 第一幕：准备工作

::: {.callout-note title="第一步：请求AI编写准备代码" icon="fas fa-cogs"}
**👤 你的指令:**

> "你好AI。我需要训练一个奖励模型（RM）。请帮我编写一段Python脚本，完成训练前的所有准备工作：
>
> 1.  **加载SFT模型**: 加载我们上一章训练好的SFT模型（从`./sft_bean_voyage_output/final`路径）作为奖励模型的骨架。注意，这次我们需要使用`AutoModelForSequenceClassification`来加载它，并明确设置`num_labels=1`，因为它只需要输出一个单一的奖励分数。
> 2.  **加载Tokenizer**: 为模型加载对应的Tokenizer。
> 3.  **创建偏好数据集**:
>     -   为"咖啡豆奇旅"项目创建一个偏好数据集。数据集是一个列表，每个元素是一个字典，包含`question`, `chosen` (我们偏好的、有品味的回答), 和`rejected` (我们不喜欢的、平庸的回答) 三个键。
>     -   使用`datasets.from_list`将其转换为Hugging Face数据集。
> 4.  **预处理数据集**: 编写一个预处理函数，将`question`和`chosen`/`rejected`回答拼接成完整的输入文本，并通过Tokenizer转换为模型可接受的`input_ids`和`attention_mask`。
>
> 请为整个脚本提供清晰的注释。"
:::

---

### 第二幕：配置并启动训练

::: {.callout-note title="第二步：请求AI配置并运行训练" icon="fas fa-sliders-h"}
**👤 你的指令:**

> "准备工作完成！现在请继续编写脚本，来配置和启动`RewardTrainer`：
>
> 1.  **配置训练参数**: 创建一个`transformers.TrainingArguments`实例，为RM训练设置合适的参数（如学习率、批次大小、训练步数等）。
> 2.  **创建RewardTrainer**: 初始化`trl.RewardTrainer`，将模型、训练参数、Tokenizer和处理过的数据集都传递给它。
> 3.  **启动训练与保存**: 调用`trainer.train()`方法启动训练，并在完成后将训练好的"品味裁判"模型保存下来。"
:::

---

### 第三幕：验证"品味裁判"

::: {.callout-note title="第三步：请求AI验证RM模型" icon="fas fa-check-circle"}
**👤 你的指令:**

> "训练完成后，我们需要验证一下我们的'品味裁判'是否真的学会了我们的偏好。请添加最后的代码来完成验证：
>
> 1.  **准备测试样本**: 定义一个问题，以及一个好的回答 (`good_response`) 和一个坏的回答 (`bad_response`)。
> 2.  **获取评分**: 使用我们训练好的`model`来分别预测这两个回答的分数。
> 3.  **判断结果**: 打印出好回答和坏回答各自获得的分数，并用一个if-else语句判断我们的"裁判"是否成功地给了好回答更高的分数。"
:::

---

现在，是时候让你亲自上场，扮演"AI训练师"的角色了。

请打开你的AI编程环境，遵循我们刚刚设计的三幕剧本，与你的AI助手合作，一步步地创建数据集、配置训练并最终验证你的"品味裁判"。

享受这个将主观"品味"量化为客观"分数"的神奇过程吧！


---

::: {.callout-caution title="AI协同工具箱" icon="fas fa-toolbox"}

### **问题："构思'Chosen'和'Rejected'回答太难了，如何批量生产？"**

**答案：** 你的感觉完全正确。

手动编写高质量的偏好对 `(chosen, rejected)` 是一件极具挑战性且耗时的工作。你需要精心设计一个"足够好"的回答，同时还要构思一个"有缺陷、但不能错得太离谱"的回答。

这正是AI协同再次大放异彩的地方。我们可以设计一个巧妙的指令，让一个强大的AI模型（如GPT-4，Claude 3）同时扮演"优秀客服"和"平庸客服"两个角色，为我们批量生产偏好数据。

**一个好的Prompt应该是这样的：**
> "你好，你现在将扮演一个双重角色，为我生成用于训练奖励模型（RM）的偏好数据集。我正在为精品咖啡品牌"咖啡豆奇旅"优化客服AI。
>
> **你的任务是：** 针对同一个用户问题，同时生成一个"金牌客服"的回答和一个"普通客服"的回答。
>
> **请严格遵循以下要求：**
> 1.  **输出格式：** 针对每一个问题，都生成一个包含 `question`, `chosen`, `rejected` 三个键的JSON对象。
> 2.  **角色定义：**
>     *   **`chosen` (金牌客服):** 这个回答必须体现出我们品牌的风格：热情、专业、有见地、有温度。它不仅要回答问题，还要能预测用户的潜在需求，提供额外的价值。
>     *   **`rejected` (普通客服):** 这个回答不能是完全错误的，但必须是平庸的、缺乏亮点的。它可以是以下几种情况之一：过于简短、答非所问、语气冷漠、只是复述了用户的问题、或者给出了一个正确但毫无帮助的"维基百科式"答案。
> 3.  **多样性：** `question`需要覆盖不同方面，例如产品推荐、咖啡知识、订单问题等。
> 4.  **高质量范例：** 请参考我提供的这条范例，学习并模仿其精髓。
>     *   **question:** "我不太懂咖啡，有什么推荐吗？"
>     *   **chosen:** "完全没问题！很高兴能带您开启咖啡探索之旅。为了给您最好的推荐，能稍微分享一下您平常喜欢喝什么吗？比如，是喜欢茶、果汁还是牛奶？这样我能更好地判断您可能喜欢的风味。如果您想直接尝试，我个人非常推荐我们的'奇旅拼配'，它的口感非常平衡，像巧克力一样顺滑，是很多新朋友的最爱。"
>     *   **rejected:** "我们有很多种咖啡豆，风味各不相同，你可以看看产品列表。"
>
> 请开始为我生成20组这样的偏好数据。"

通过这样的指令，AI将成为你的"偏好数据生成引擎"，为你源源不断地创造训练"品味裁判"所需的宝贵"录像带"，将你从繁重的人工标注中解放出来。

::: 

## 本节小结

### 🎯 核心收获
- **端到端RM训练流程**: 你亲手实践了使用`trl`库的`RewardTrainer`训练奖励模型的全过程，包括构建偏好数据集、加载正确的模型架构 (`AutoModelForSequenceClassification`)，以及启动训练和验证。
- **偏好即监督**: 你深刻地体会到，奖励模型的核心监督信号来自于成对的偏好数据 (`chosen` vs `rejected`)，而非单一的标准答案。
- **AI协同生成偏好数据**: 你学会了一种极其强大的AI协同技术——通过精心设计的Prompt，让一个LLM扮演多重角色，为你批量生成高质量、有细微差别的偏好数据集。

### 🤔 下一步的思考
- **"裁判"的价值**: 我们现在有了一个能给答案打"品味分"的裁判模型（RM），但它本身并不能直接提升我们"金牌客服"（SFT模型）的回答能力。我们如何利用这个"裁判"的打分，来驱动我们的"运动员"进行自我提升，创造出更高分的回答呢？
- **对齐的最后一步**: 我们已经完成了"岗前培训" (SFT) 和"裁判培养" (RM)。现在，万事俱备，只欠一场真正的"奥运会"了。在这场"奥运会"上，我们的AI运动员需要在裁判的实时评分引导下，不断挑战更高难度的动作。这最后一步该如何实现？

准备好迎接对齐工程三部曲的最终章，也是最高潮的部分——强化学习（PPO/DPO）了吗？ 