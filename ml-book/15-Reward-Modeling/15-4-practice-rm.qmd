# 15.4 Practice: 训练"咖啡品味裁判"

理论和方法都已清晰，现在是我们亲手"培养"出"咖啡豆奇旅"项目专属的"品味裁判"的时刻了。

我们将再次通过一个完整的"指令剧本"，指挥AI使用`trl`库中的`RewardTrainer`，完成从数据准备、模型加载到训练和验证的全过程。

---

## AI协同实践：一个完整的RM训练指令剧本

### 第一幕：准备工作

::: {.callout-note title="第一步：请求AI编写准备代码" icon="fas fa-cogs"}
**👤 你的指令:**

> "你好AI。我需要训练一个奖励模型（RM）。请帮我编写一段Python脚本，完成训练前的所有准备工作：
>
> 1.  **加载SFT模型**: 加载我们上一章训练好的SFT模型（从`./sft_bean_voyage_output/final`路径）作为奖励模型的骨架。注意，这次我们需要使用`AutoModelForSequenceClassification`来加载它，并明确设置`num_labels=1`，因为它只需要输出一个单一的奖励分数。
> 2.  **加载Tokenizer**: 为模型加载对应的Tokenizer。
> 3.  **创建偏好数据集**:
>     -   为"咖啡豆奇旅"项目创建一个偏好数据集。数据集是一个列表，每个元素是一个字典，包含`question`, `chosen` (我们偏好的、有品味的回答), 和`rejected` (我们不喜欢的、平庸的回答) 三个键。
>     -   使用`datasets.from_list`将其转换为Hugging Face数据集。
> 4.  **预处理数据集**: 编写一个预处理函数，将`question`和`chosen`/`rejected`回答拼接成完整的输入文本，并通过Tokenizer转换为模型可接受的`input_ids`和`attention_mask`。
>
> 请为整个脚本提供清晰的注释。"
:::

---

### 第二幕：配置并启动训练

::: {.callout-note title="第二步：请求AI配置并运行训练" icon="fas fa-sliders-h"}
**👤 你的指令:**

> "准备工作完成！现在请继续编写脚本，来配置和启动`RewardTrainer`：
>
> 1.  **配置训练参数**: 创建一个`transformers.TrainingArguments`实例，为RM训练设置合适的参数（如学习率、批次大小、训练步数等）。
> 2.  **创建RewardTrainer**: 初始化`trl.RewardTrainer`，将模型、训练参数、Tokenizer和处理过的数据集都传递给它。
> 3.  **启动训练与保存**: 调用`trainer.train()`方法启动训练，并在完成后将训练好的"品味裁判"模型保存下来。"
:::

---

### 第三幕：验证"品味裁判"

::: {.callout-note title="第三步：请求AI验证RM模型" icon="fas fa-check-circle"}
**👤 你的指令:**

> "训练完成后，我们需要验证一下我们的'品味裁判'是否真的学会了我们的偏好。请添加最后的代码来完成验证：
>
> 1.  **准备测试样本**: 定义一个问题，以及一个好的回答 (`good_response`) 和一个坏的回答 (`bad_response`)。
> 2.  **获取评分**: 使用我们训练好的`model`来分别预测这两个回答的分数。
> 3.  **判断结果**: 打印出好回答和坏回答各自获得的分数，并用一个if-else语句判断我们的"裁判"是否成功地给了好回答更高的分数。"
:::

---

现在，是时候让你亲自上场，扮演"AI训练师"的角色了。

请打开你的AI编程环境，遵循我们刚刚设计的三幕剧本，与你的AI助手合作，一步步地创建数据集、配置训练并最终验证你的"品味裁判"。

享受这个将主观"品味"量化为客观"分数"的神奇过程吧！ 