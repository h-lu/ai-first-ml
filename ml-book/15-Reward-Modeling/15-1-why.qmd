# 15.1 Why: 为何"标准答案"远远不够？

SFT（监督微调）是一个强大的工具，它基于一个简单而有效的前提：**高质量的范例带来高质量的模仿**。

但这引出了一个根本性的问题：**对于很多现实世界的问题，高质量的"唯一标准答案"真的存在吗？**

让我们回到"咖啡豆奇旅"的客服场景。当顾客问"有什么推荐？"时，一个好的客服真的只有一个标准话术吗？

*   **场景一：** 面对一位行色匆匆的上班族，最好的回答可能是简洁、直接、强调提神效果。
*   **场景二：** 面对一位周末来放松的咖啡爱好者，最好的回答可能是详细介绍风味、产地和冲煮故事。
*   **场景三：** 面对一位表示"从没喝过手冲"的顾客，最好的回答可能是用生动的类比来打消他的疑虑。

这些回答可能都同样"好"，但它们好得各不相同。我们很难将它们全部写入一个"标准答案手册"中让SFT去模仿。更糟糕的是，如果我们强行选择一个"中庸"的回答作为标准答案，训练出的模型也必然是中庸的。

更进一步，很多时候我们追求的并非"正确性"，而是一些更模糊、更主观的品质，比如：

*   **热情 (Helpfulness):** 回答是否积极主动地解决了用户的潜在问题？
*   **无害性 (Harmlessness):** 回答是否包含了任何不恰当、有偏见或危险的内容？
*   **简洁性 (Coniseness):** 回答是否言简意赅，直击要点，没有废话？
*   **品牌风格 (Brand Voice):** 回答的语气是否符合我们"咖啡豆奇旅"的品牌形象？

这些品质，几乎不可能通过一个"标准答案"来定义和教会。**SFT能教会模型"做对题"，但很难教会它"如何优雅地、创造性地、有品味地做对题"。**

这就是我们需要引入一种全新范式的原因。我们不再试图定义那个完美的"标准答案"，而是退后一步，采取一种更符合人类直觉的方式：**比较与偏好**。我们只需要向AI展示我们的选择："在这两个回答中，我更喜欢这一个。"

这种从"绝对指令"到"相对偏好"的转变，正是奖励建模（Reward Modeling）将要为我们开启的、通往更高AI智能的大门。 