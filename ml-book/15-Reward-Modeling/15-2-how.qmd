# 15.2 How: 如何量化"品味"？

我们已经确定，要教会AI"品味"，关键在于向它展示我们的"偏好"。但这又带来一个新问题：机器只能理解数字，我们该如何将"我更喜欢A而不是B"这种主观感受，转化成一个能让机器学习的数学模型呢？

让我们再次与AI合作，进行一场关于"如何量化品味"的头脑风暴。

::: {.callout-note title="与AI的探索之旅"}
**你：** 你好AI，我现在面临一个挑战。对于同一个问题，我有两个不同的回答，一个"好"的（chosen），一个"差"的（rejected）。我想让模型学会这种偏好，我该怎么做？

**AI助手：** 这是一个非常棒的问题！我们的目标是创建一个系统，它能自动给任何一个回答打分，并且保证"好"的回答得分，永远高于"差"的回答得分。你同意这个目标吗？

**你：** 同意！这听起来就像是为我们的客服回答建立一个"评分系统"。得分高的就是好回答，得分低的就是差回答。

**AI助手：** 正是如此！这个"评分系统"在我们的领域里，就叫做**奖励模型（Reward Model, RM）**。它的本质，就是一个接收文本（问题+回答），输出一个单一数字（分数）的模型。我们可以把它表示为 `Score = RM(Question, Answer)`。

**你：** 好的，那我们如何训练这个`RM`呢？它一开始也不知道该给谁高分，给谁低分。

**AI助手：** 这就是"偏好数据"发挥作用的地方了。对于每一组 `(Question, Chosen Answer, Rejected Answer)` 数据，我们都在告诉模型一个明确的不等式：
`RM(Question, Chosen Answer) > RM(Question, Rejected Answer)`
我们的训练目标，就是调整`RM`的内部参数，让这个不等式在我们的整个偏好数据集中尽可能地成立。

**你：** 我好像有点明白了。这听起来像一个……排序问题？或者说，一个特殊的分类问题？我不需要模型算出某个回答的具体分数，比如87分还是92分，我只需要它能正确地判断出"A比B好"就行。

**AI助手：** 你的直觉非常敏锐！这正是RM训练的精髓。在实践中，我们通常使用一个叫`RewardTrainer`的工具。我们把成对的`(Chosen Answer, Rejected Answer)`喂给它，它的底层损失函数会自动处理这个">"关系，惩罚那些把"差"的回答排在"好"的回答前面的模型行为。通过成千上万次这样的"偏好判断"训练，这个RM模型就逐渐内化了我们的"品味"，变成了一个可靠的"品味裁判"。

**你：** 太酷了！所以，我们接下来的任务就是：
1.  创建一批"咖啡豆奇旅"的偏好数据集，包含"好客服"和"平庸客服"的回答对比。
2.  使用`RewardTrainer`，来训练我们的"咖啡品味裁判"。
对吗？

**AI助手：** 完全正确！你已经掌握了奖励建模的核心工作流程。让我们开始动手吧！
::: 