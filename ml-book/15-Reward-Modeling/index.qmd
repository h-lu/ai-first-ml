# 第15章 第二步：奖励建模 —— 教会AI拥有"品味"


在上一章，我们成功地为"咖啡豆奇旅"的AI客服进行了一次卓有成效的"岗前培训"（SFT）。现在，我们的AI已经能像模像样地回答关于我们产品的专业问题，说话的风格也初步具备了"金牌客服"的雏形。

但这通常会引出一个更深、也更有趣的问题。

我们通过SFT教会了模型"**说什么**"，但我们还没有教会它"**应该怎样说**"。我们教会了它"事实"，但还没有教会它"**品味**"。

想象一下，对于"有什么推荐的咖啡豆吗？"这个问题，我们的SFT模型可能会给出一个"标准且安全"的回答：
> "我们有多款咖啡豆，例如耶加雪菲和曼特宁，您可以根据自己的喜好选择。"

这个回答没有错，但它就像一杯白开水，缺乏魅力和灵魂。我们真正想要的"金牌客服"回答是这样的：
> "当然！如果您是第一次尝试，我强烈推荐我们的'奇旅拼配'！它就像一杯'可以喝的巧克力坚果棒'，风味稳定，特别适合搭配牛奶。保证能给您一个惊喜！"

后者充满了热情、见地和感染力。我们作为人类，凭直觉就能判断出第二个回答远胜于第一个。

**但AI如何习得这种"品味"和"直觉"呢？**

本章，我们将学习一种更高级的AI对齐技术：**奖励建模（Reward Modeling, RM）**。我们将不再为AI提供唯一的"标准答案"，而是扮演一位"品味导师"或"美食评论家"，不断地向它展示我们的**偏好**——"这个回答更好"、"那个回答不行"。

通过这个过程，我们将训练出一个专门的"**品味裁判**"模型。这个模型本身不负责回答问题，它的唯一任务，就是给任何一个回答打一个"品味分"，判断其"好坏"程度。

这个"品味裁判"，将是我们下一章驱动AI自我进化的关键。准备好，从"教导"AI，升级为"塑造"AI的品味了吗？ 