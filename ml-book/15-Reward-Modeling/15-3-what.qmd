# 15.3 What: 核心概念：奖励建模 (RM)

::: {.callout-tip title="核心概念：奖励建模 (Reward Modeling)"}

**一句话定义：**
奖励建模（RM）是一种训练"裁判"模型的技术，这个模型不直接回答问题，而是学会给任何一个"问题-回答"对打一个分数，这个分数代表了该回答在多大程度上符合人类的偏好和价值观。

---

**生动的类比："奥运跳水裁判"的养成**

想象一下，我们要培养一位顶级的奥运会跳水比赛裁判。我们该怎么做？

1.  **挑选"裁判苗子"：**
    我们不会找一个完全不懂体育的人。我们会找一个有一定基础的人，比如一位退役的跳水运动员。他知道跳水的基本动作和规则。在我们的世界里，这个"裁判苗子"就是我们上一章**SFT过的模型**。它已经被"培训"过，了解我们的业务（咖啡知识），具备了基础的判断能力。

2.  **进行"裁判培训"：**
    我们不会直接告诉他"一个完美的10分跳水是怎样的"，因为完美的动作很难用语言精确描述。相反，我们会给他播放成千上万段比赛录像。每一段录像都包含**两位选手**的跳水动作（选手A和选手B）。我们只需要告诉这位准裁判一个最简单的信息："**在这两个人里，选手A跳得更好。**"

    *   **录像对 (Preference Pair):** 选手A的动作 (`chosen`) vs 选手B的动作 (`rejected`)。
    *   **培训数据 (Preference Dataset):** 成千上万这样的"A比B好"的比赛录像对。

3.  **形成"打分直觉"：**
    在观看了海量的"A比B好"的录像后，这位准裁判的大脑里会逐渐形成一种深刻的、内化的"**打分直觉**"。他开始理解什么是"水花压得好"，什么是"空中姿态优美"，什么是"动作有难度"。

    最终，他成了一位真正的裁判。现在，随便给他看一段新的跳水录像，即使他以前从未见过，他也能凭借自己已经形成的"打分直-觉"，给出一个相当精确的分数（例如，8.7分）。

    这个过程，就是**奖励建模（Reward Modeling）**。
    *   **裁判模型 (Reward Model):** 最终学会打分的裁判。
    *   **偏好数据 (Preference Data):** 包含 `(chosen, rejected)` 对的训练数据。
    *   **奖励/分数 (Reward/Score):** 裁判模型对一个新回答给出的"品味分"。

这个"裁判"本身不参加跳水比赛（不生成回答），它的唯一使命，就是为后续参加比赛的"运动员"（我们将在下一章用PPO/DPO训练的模型）提供公正、准确的评分，引导他们跳出（生成）更精彩的动作（回答）。

---

**核心工具：TRL `RewardTrainer`**

Hugging Face的`trl`库为这个"裁判培训"过程，提供了核心工具：`RewardTrainer`。它像一位经验丰富的"**裁判长**"，你只需要把"裁判苗子"（SFT模型）和大量的"比赛录像对"（偏好数据集）交给他，他就能高效地训练出你想要的"奥运跳水裁判"（奖励模型）。
::: 